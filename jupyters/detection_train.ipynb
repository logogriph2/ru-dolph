{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ..\n",
    "!pip install datasets\n",
    "!pip install pycocotools\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbc3855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rudalle import get_vae\n",
    "from rudalle.utils import seed_everything\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'ru-dolph')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchvision\n",
    "from rudalle.image_prompts import ImagePrompts\n",
    "from rudolph.model import get_rudolph_model, ruDolphModel, FP16Module\n",
    "from rudolph import utils\n",
    "from rudolph.model.utils import get_attention_mask\n",
    "from rudalle import get_tokenizer, get_vae\n",
    "from rudolph.api import ruDolphApi\n",
    "\n",
    "import datasets\n",
    "import random\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Any, Callable, Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abbbcb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5314b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2bc755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13149218",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed08829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW VERSION!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-rudolph-obj-detection-0/lib/python3.7/site-packages/huggingface_hub/file_download.py:632: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = get_rudolph_model('1.3B', pretrained=True, fp16=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6f23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer --> ready\n",
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n",
      "vae --> ready\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "vae = get_vae(dwt=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af9ba5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, model, checkpoint_path):\n",
    "        self.device = model.get_param('device')\n",
    "        self.l_text_seq_length = model.get_param('l_text_seq_length')\n",
    "        self.r_text_seq_length = model.get_param('r_text_seq_length')\n",
    "        self.image_tokens_per_dim = model.get_param('image_tokens_per_dim')\n",
    "        self.image_seq_length = model.get_param('image_seq_length')\n",
    "        self.epochs = 1\n",
    "        self.save_path= checkpoint_path\n",
    "        self.model_name = 'rudolph_object_detection_grounding_neg_sampling_coco_vg_nmsn_'\n",
    "        self.save_every = 10000\n",
    "        self.bs = 1\n",
    "        self.clip = 1.0\n",
    "        self.lr = 2e-5\n",
    "        self.wandb = False\n",
    "        self.lt_loss_weight = 0.01\n",
    "        self.img_loss_weight = 0.01\n",
    "        self.rt_loss_weight = 7\n",
    "        self.loc_loss_weight = 0.05\n",
    "        self.category_weight = 0\n",
    "        self.bin_size = 8\n",
    "        self.nms_loss_weight=0.1\n",
    "        self.categories_num=2\n",
    "        self.iou_threshold=0.3\n",
    "        self.image_size = self.image_tokens_per_dim * self.bin_size\n",
    "        self.conf_loss_weight=0\n",
    "        self.cat_idx=4\n",
    "        \n",
    "checkpoint_path = '../model/checkpoints/'\n",
    "args = Args(model, checkpoint_path)\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d1b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29335957",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPC_TOKENS = {\n",
    "    '<LT_UNK>': 16384,\n",
    "    '<RT_UNK>': 16385,\n",
    "    '<LT_T2I>': 16386,\n",
    "    '<LT_I2T>': 16387,\n",
    "    '<LT_T2T>': 16388,\n",
    "    '<RT_I2T>': 16389,\n",
    "    \n",
    "    '<LT_TQA>': 16390,\n",
    "    '<RT_TQA>': 16391,\n",
    "    '<LT_ODG>': 16392,\n",
    "    '<RT_ODG>': 16393,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bb85d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(args.image_tokens_per_dim):\n",
    "    n_token = SPC_TOKENS['<RT_ODG>']+1+n\n",
    "    SPC_TOKENS['<RT_OD'+str(n)+'>'] = n_token\n",
    "last_spc_token = n_token\n",
    "\n",
    "SPC_TOKENS['<RT_NB>']= last_spc_token+1\n",
    "SPC_TOKENS['<RT_NL>']= last_spc_token+2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec34edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = T.Compose([\n",
    "    T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    T.ToTensor(),\n",
    "    T.Resize((args.image_size, args.image_size))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9aef051",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = np.load('../../data/coco_exclude.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e700230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoVisualGenomeDataset(Dataset):\n",
    "    spc_id = -1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        rus_categories_list,\n",
    "        coco_exclude_list,\n",
    "        coco_root: str,\n",
    "        coco_ann_file: str,\n",
    "        vg_annotation_path: str,\n",
    "        vg_images_root: str,\n",
    "        args,\n",
    "        image_transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.coco_root = coco_root\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "        self.image_size = args.image_size\n",
    "        self.bin_size = args.bin_size\n",
    "        self.r_text_seq_length = args.r_text_seq_length\n",
    "        self.l_text_seq_length = args.l_text_seq_length\n",
    "        self.image_tokens_per_dim = args.image_tokens_per_dim\n",
    "        \n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_eos_id = tokenizer.eos_id\n",
    "        self.tokenizer_bos_id = tokenizer.bos_id\n",
    "        self.spc_tokens = SPC_TOKENS\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        text_prompt = 'найди на картинке все '\n",
    "        \n",
    "        print('LOAD COCO')\n",
    "\n",
    "        self.coco = COCO(coco_ann_file)\n",
    "        self.idxs = list(sorted(self.coco.imgs.keys()))\n",
    "        for ex_img in coco_exclude_list:\n",
    "            self.idxs.remove(ex_img)\n",
    "         \n",
    "        \n",
    "        categories = self.coco.dataset['categories']\n",
    "        cat_idxs = list([cat['id'] for cat in categories])\n",
    "        self.category_encoding_map = {}\n",
    "        for i, cat in enumerate(categories):\n",
    "            self.category_encoding_map[cat['id']] = self.encode_text(text=text_prompt+rus_categories_list[i],\n",
    "                                                                text_seq_length=args.l_text_seq_length)\n",
    "        \n",
    "        \n",
    "        for idx in self.idxs:\n",
    "            anns = self.coco.loadAnns(self.coco.getAnnIds(idx))\n",
    "            anns_df = pd.DataFrame(anns)\n",
    "            if len(anns_df.columns)>0:\n",
    "                cat_id_set = set(anns_df['category_id'])\n",
    "                for cat_id in cat_id_set:\n",
    "                    coco_image = self.coco.loadImgs(idx)[0][\"file_name\"]\n",
    "                    new_sample = {'encoded_left':self.category_encoding_map[cat_id],\n",
    "                                 'bboxes':anns_df[anns_df['category_id']==cat_id]['bbox'].tolist(),\n",
    "                                'image_address':os.path.join(self.coco_root, coco_image)}\n",
    "                    self.data.append(new_sample)\n",
    "                    \n",
    "        print('LOAD VISUAL GENOME')\n",
    "        \n",
    "        self.vg_images_root = vg_images_root\n",
    "        with open(vg_annotation_path, 'rb') as fj:\n",
    "            vg_annotation = json.load(fj)\n",
    "        \n",
    "        for image_name in vg_annotation.keys():\n",
    "            if image_name in os.listdir(self.vg_images_root):\n",
    "                ann_img = vg_annotation[image_name]\n",
    "                for phrase in ann_img.keys():\n",
    "                    new_sample = {'encoded_left':self.encode_text(text=text_prompt+phrase,\n",
    "                                                                text_seq_length=args.l_text_seq_length),\n",
    "                                 'bboxes':[ann_img[phrase][0]],\n",
    "                                'image_address':os.path.join(self.vg_images_root, image_name)}\n",
    "                    self.data.append(new_sample)\n",
    "            \n",
    "    def _load_image(self, image_address) -> Image.Image:\n",
    "        return Image.open(image_address)\n",
    "    \n",
    "    def resize_bbox(self, bbox, img_size):\n",
    "        \n",
    "        bbox_x0 = bbox[0]\n",
    "        bbox_y0 = bbox[1]\n",
    "        bbox_x1 = bbox_x0+bbox[2]\n",
    "        bbox_y1 = bbox_y0+bbox[3]\n",
    "        \n",
    "        img_w = img_size[0]\n",
    "        img_h = img_size[1]\n",
    "        \n",
    "        max_wh = max(img_w,img_h)\n",
    "\n",
    "        resized_x0 = int(bbox_x0*self.image_size/img_w)\n",
    "        resized_y0 = int(bbox_y0*self.image_size/img_h)\n",
    "        resized_x1 = int(bbox_x1*self.image_size/img_w)\n",
    "        resized_y1 = int(bbox_y1*self.image_size/img_h)\n",
    "        return [resized_x0,resized_y0,resized_x1, resized_y1]\n",
    "    \n",
    "    def encode_bbox(self, resized_bbox):\n",
    "        \n",
    "        bbox_x0_bin = resized_bbox[0]//self.bin_size\n",
    "        bbox_y0_bin = resized_bbox[1]//self.bin_size\n",
    "        bbox_x1_bin = min(resized_bbox[2]//self.bin_size, self.image_tokens_per_dim-1)\n",
    "        bbox_y1_bin = min(resized_bbox[3]//self.bin_size, self.image_tokens_per_dim-1)\n",
    "        \n",
    "        new_bbox_tokens = [SPC_TOKENS['<RT_OD'+str(bbox_x0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_x1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y1_bin)+'>']]\n",
    "        \n",
    "        return new_bbox_tokens\n",
    "    \n",
    "    def create_new_box(self):\n",
    "        bbox_x0_bin = random.randint(0, self.args.image_tokens_per_dim-3)\n",
    "        bbox_y0_bin = random.randint(0, self.args.image_tokens_per_dim-3)\n",
    "        bbox_x1_bin = random.randint(bbox_x0_bin, self.args.image_tokens_per_dim-1)\n",
    "        bbox_y1_bin = random.randint(bbox_y0_bin, self.args.image_tokens_per_dim-1)\n",
    "        new_bbox_tokens = [SPC_TOKENS['<RT_OD'+str(bbox_x0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_x1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_NL>']\n",
    "                          ]\n",
    "        return new_bbox_tokens\n",
    "    \n",
    "    def encode_target(self, bboxes, img_size):\n",
    "        encoded_target = [self.tokenizer_bos_id, SPC_TOKENS['<RT_ODG>']]\n",
    "        ground_truth = []\n",
    "        \n",
    "        for i,bbox in enumerate(bboxes):\n",
    "            \n",
    "            resized_bbox = self.resize_bbox(bbox, img_size)\n",
    "            encoded_bbox = self.encode_bbox(resized_bbox)\n",
    "            \n",
    "            ground_truth += resized_bbox\n",
    "            ground_truth += [0]\n",
    "            \n",
    "            for loc_token in encoded_bbox:\n",
    "                encoded_target.append(loc_token)\n",
    "            encoded_target.append(SPC_TOKENS['<RT_NB>'])\n",
    "        for k in range((self.r_text_seq_length-2-len(encoded_target))//5+1):\n",
    "            encoded_target += self.create_new_box()\n",
    "            \n",
    "        encoded_target.append(SPC_TOKENS['<RT_NL>'])\n",
    "        \n",
    "        return torch.Tensor(encoded_target), torch.Tensor(ground_truth)\n",
    "    \n",
    "    def encode_text(self, text, text_seq_length, add_special = True):\n",
    "        tokens = self.tokenizer.tokenizer.encode([text], output_type=yttm.OutputType.ID)[0]\n",
    "        bos = [self.tokenizer.bos_id]\n",
    "        if add_special:\n",
    "            bos.append(self.spc_id)\n",
    "        tokens = bos + tokens + [self.tokenizer.eos_id]\n",
    "        return self.tokenizer.prepare_tokens(tokens, text_seq_length)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = self.data[index]\n",
    "        left_encoded_text = sample['encoded_left']\n",
    "        bboxes = sample['bboxes']\n",
    "        image_address = sample['image_address']\n",
    "        \n",
    "        image = self._load_image(image_address)\n",
    "        img_size = image.size\n",
    "        transformed_image = self.image_transform(image)\n",
    "        \n",
    "        left_special_token = '<LT_ODG>'\n",
    "        right_special_token = '<RT_ODG>'\n",
    "        \n",
    "        left_encoded_text[torch.where(left_encoded_text == self.spc_id)] = self.spc_tokens[left_special_token]\n",
    "        \n",
    "        encoded_target, ground_truth = self.encode_target(bboxes, img_size)\n",
    "\n",
    "        return left_encoded_text, transformed_image, encoded_target, ground_truth    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "038de09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_rus_names = [\"человек\",\"велосипед\",\"автомобиль\",\"мотоцикл\",\"самолёт\",\"автобус\",\"поезд\",\"грузовик\",\"лодка\",\n",
    "                 \"светофор\",\"гидрант\",\"знак стоп\",\"парковочный счетчик\",\"скамейка\",\"птица\",\"кошка\",\"собака\",\n",
    "                \"лошадь\",\"овца\",\"корова\",\"слон\",\"медведь\",\"зебра\",\"жираф\",\"рюкзак\",\"зонт\",\"сумочка\",\"галстук\",\n",
    "                \"чемодан\",\"фрисби\",\"лыжи\",\"сноуборд\",\"спортивный мяч\",\"воздушный змей\",\"бейсбольная бита\",\n",
    "                 \"бейсбольная перчатка\",\"скейтборд\",\"доска для сёрфинга\",\"теннисная ракетка\",\"бутылка\",\"бокал для вина\",\n",
    "                 \"чашка\",\"вилка\",\"нож\",\"ложка\",\"миска\",\"банан\",\"яблоко\",\"сэндвич\",\"апельсин\",\"брокколи\",\"морковь\",\n",
    "                 \"ход-дог\",\"пицца\",\"пончик\",\"торт\",\"стул\",\"диван\",\"растение в горшке\",\"кровать\",\"обеденный стол\",\"туалет\",\"телевизор\",\n",
    "                 \"ноутбук\",\"компьютерная мышь\",\"пульт\",\"клавиатура\",\"мобильный телефон\",\"микроволновка\",\"духовка\",\n",
    "                 \"тостер\",\"раковина\",\"холодильник\",\"книга\",\"часы\",\"ваза\",\"ножницы\",\"плюшевый мишка\",\"фен\",\"зубная щётка\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e22bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD COCO\n",
      "loading annotations into memory...\n",
      "Done (t=13.43s)\n",
      "creating index...\n",
      "index created!\n",
      "LOAD VISUAL GENOME\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CocoVisualGenomeDataset(coco_root=r'../../data/train2017',\n",
    "                             coco_ann_file=r'../../data/annotations/instances_train2017.json',\n",
    "                             image_transform=image_transform,\n",
    "                             tokenizer=tokenizer,\n",
    "                             args = args,\n",
    "                             coco_exclude_list=exclude_list,\n",
    "                             rus_categories_list=cat_rus_names,\n",
    "                                      vg_annotation_path=r'../../data/vg_intersection_rus_train.json',\n",
    "                                      vg_images_root = r'../../data/VG_100K'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26dc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "254a9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_right(right_tokens, length=None, scores=True):\n",
    "    boxes = torch.zeros((length, 4))\n",
    "    labels = torch.ones(length)\n",
    "    scores = torch.zeros(length)\n",
    "    print('right_tokens',right_tokens.int())\n",
    "    \n",
    "    print('len(right_tokens)',len(right_tokens),len(right_tokens)//5)\n",
    "    \n",
    "    for k in range(length):\n",
    "        boxes[k,:] = right_tokens[k*5:(k+1)*5-1].unsqueeze(0).cpu()\n",
    "        scores[k] = 1\n",
    "        \n",
    "    boxes -= SPC_TOKENS['<RT_OD0>']*torch.ones((length, 4))\n",
    "    boxes *= args.bin_size\n",
    "\n",
    "    return [{'boxes':boxes, 'labels':labels, 'scores':scores}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e9ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(tokenizer, encoded, ignore_ids):\n",
    "    return tokenizer.tokenizer.decode(encoded.cpu().numpy().tolist(), ignore_ids=ignore_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540514bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_ids = [\n",
    "    tokenizer.eos_id, tokenizer.bos_id, tokenizer.unk_id, tokenizer.pad_id,\n",
    "    -1, *list(SPC_TOKENS.values())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8e5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=args.bs, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8cb154",
   "metadata": {},
   "outputs": [],
   "source": [
    "left, img, tgt = train_dataset.__getitem__(len(train_dataset)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "353fb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False,\n",
    "):\n",
    "    for name, p in model.module.named_parameters():\n",
    "        name = name.lower()\n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            p.requires_grad = not freeze_ff\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f3a9077",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../model/checkpoints/rudolph_object_detection_grounding_neg_sampling_coco_vg_nmsn_02_rudolph_200000.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59ed0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=args.lr, final_div_factor=500, \n",
    "    steps_per_epoch=len(train_dataloader), epochs=args.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a45fa018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, args: Args, train_dataloader, from_checkpoints = False):\n",
    "    \"\"\"\n",
    "      args - arguments for training\n",
    "\n",
    "      train_dataloader - VisualGenomeDataset\n",
    "      \"\"\"\n",
    "    loss_logs = []\n",
    "    encoded_right_text = None\n",
    "    #try:\n",
    "    t_steps = len(train_dataloader)*args.epochs\n",
    "    progress = tqdm(total = t_steps, desc='🦌🦌🦌finetuning process🦌🦌🦌')\n",
    "\n",
    "    save_counter = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    if from_checkpoints:\n",
    "        save_counter = max([int(f.split('_')[-1].split('.')[0]) for f in os.listdir(args.save_path) if f.startswith(args.model_name)])\n",
    "        model, optimizer, start_epoch, loss_logs = load_checkpoint(model, optimizer, loss_logs, filename=os.path.join(args.save_path,f\"{args.model_name}state_{save_counter}.pt\"))\n",
    "        start_epoch -= 1\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "        for encoded_left_text, images, encoded_right_text, _ in train_dataloader: #, prompt, image, locs\n",
    "\n",
    "            bs_text = encoded_right_text.shape[0]\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            image_seq_length = args.image_tokens_per_dim ** 2\n",
    "            total_seq_length = args.l_text_seq_length + image_seq_length + args.r_text_seq_length\n",
    "            encoded_images = vae.get_codebook_indices(images, disable_gumbel_softmax=True)\n",
    "\n",
    "            attention_mask_text = get_attention_mask(bs_text, args.l_text_seq_length,\n",
    "                                             args.image_tokens_per_dim, \n",
    "                                             args.r_text_seq_length, args.device)\n",
    "            \n",
    "            input_ids_text = torch.cat((encoded_left_text.to(args.device).int(),\n",
    "                            encoded_images.int(),\n",
    "                            encoded_right_text.to(args.device).int()), dim=1)\n",
    "            loss, loss_values = model.forward(input_ids_text.long(), attention_mask_text, \n",
    "                                                             lt_loss_weight=args.lt_loss_weight,\n",
    "                                                             img_loss_weight=args.img_loss_weight, \n",
    "                                                             rt_loss_weight=args.rt_loss_weight,\n",
    "                                              return_loss=True,\n",
    "                                              category_weight=args.category_weight,\n",
    "                                              fake_category_token = SPC_TOKENS['<RT_NL>'],\n",
    "                                              loc_loss_weight=args.loc_loss_weight,\n",
    "                                              zero_loc_token = SPC_TOKENS['<RT_OD0>'],\n",
    "                                              nms_loss_weight=args.nms_loss_weight,\n",
    "                                                categories_num=args.categories_num,\n",
    "                                                iou_threshold=args.iou_threshold,\n",
    "                                              cat_idx=args.cat_idx,\n",
    "                                              conf_loss_weight=args.conf_loss_weight\n",
    "                                             )\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            save_counter += 1\n",
    "            if save_counter % args.save_every == 0:\n",
    "                print(f'Saving checkpoint here {args.model_name}_rudolph_{save_counter+200000}.pt')\n",
    "                plt.plot(loss_logs)\n",
    "                plt.show()\n",
    "                state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "                         'optimizer': optimizer.state_dict(), 'losslogger': loss_logs}\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(args.save_path,f\"{args.model_name}rudolph_{save_counter+200000}.pt\")\n",
    "                )\n",
    "                torch.save(\n",
    "                    state,\n",
    "                    os.path.join(args.save_path,f\"{args.model_name}state_{save_counter+200000}.pt\")\n",
    "                )\n",
    "\n",
    "            loss_logs+=[loss.item()]\n",
    "            progress.update()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        print(f'Complitly tuned and saved here  {args.model_name}__object_detection_last.pt')\n",
    "        plt.plot(loss_logs)\n",
    "        plt.show()\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(args.save_path,f\"{args.model_name}object_detection_last.pt\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5dde99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = freeze(\n",
    "    model=model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False,\n",
    ")\n",
    "\n",
    "train(model, optimizer, scheduler, args, train_dataloader, from_checkpoints=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
