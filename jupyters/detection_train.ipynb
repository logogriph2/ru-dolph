{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ..\n",
    "!pip install datasets\n",
    "!pip install pycocotools\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbc3855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rudalle import get_vae\n",
    "from rudalle.utils import seed_everything\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'ru-dolph')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchvision\n",
    "from rudalle.image_prompts import ImagePrompts\n",
    "from rudolph.model import get_rudolph_model, ruDolphModel, FP16Module\n",
    "from rudolph import utils\n",
    "from rudolph.model.utils import get_attention_mask\n",
    "from rudalle import get_tokenizer, get_vae\n",
    "from rudolph.api import ruDolphApi\n",
    "\n",
    "import datasets\n",
    "import random\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Any, Callable, Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abbbcb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5314b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2bc755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13149218",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed08829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW VERSION!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-rudolph-obj-detection-0/lib/python3.7/site-packages/huggingface_hub/file_download.py:632: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = get_rudolph_model('1.3B', pretrained=True, fp16=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6f23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer --> ready\n",
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n",
      "vae --> ready\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "vae = get_vae(dwt=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af9ba5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, model, checkpoint_path):\n",
    "        self.device = model.get_param('device')\n",
    "        self.l_text_seq_length = model.get_param('l_text_seq_length')\n",
    "        self.r_text_seq_length = model.get_param('r_text_seq_length')\n",
    "        self.image_tokens_per_dim = model.get_param('image_tokens_per_dim')\n",
    "        self.image_seq_length = model.get_param('image_seq_length')\n",
    "        self.epochs = 1\n",
    "        self.save_path= checkpoint_path\n",
    "        self.model_name = 'rudolph_object_detection_grounding_neg_sampling_coco_vg_nmsn_'\n",
    "        self.save_every = 10000\n",
    "        self.bs = 1\n",
    "        self.clip = 1.0\n",
    "        self.lr = 2e-5\n",
    "        self.wandb = False\n",
    "        self.lt_loss_weight = 0.01\n",
    "        self.img_loss_weight = 0.01\n",
    "        self.rt_loss_weight = 7\n",
    "        self.loc_loss_weight = 0.05\n",
    "        self.category_weight = 0\n",
    "        self.bin_size = 8\n",
    "        self.nms_loss_weight=0.1\n",
    "        self.categories_num=2\n",
    "        self.iou_threshold=0.3\n",
    "        self.image_size = self.image_tokens_per_dim * self.bin_size\n",
    "        self.conf_loss_weight=0\n",
    "        self.cat_idx=4\n",
    "        \n",
    "checkpoint_path = '../model/checkpoints/'\n",
    "args = Args(model, checkpoint_path)\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d1b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29335957",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPC_TOKENS = {\n",
    "    '<LT_UNK>': 16384,\n",
    "    '<RT_UNK>': 16385,\n",
    "    '<LT_T2I>': 16386,\n",
    "    '<LT_I2T>': 16387,\n",
    "    '<LT_T2T>': 16388,\n",
    "    '<RT_I2T>': 16389,\n",
    "    \n",
    "    '<LT_TQA>': 16390,\n",
    "    '<RT_TQA>': 16391,\n",
    "    '<LT_ODG>': 16392,\n",
    "    '<RT_ODG>': 16393,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bb85d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(args.image_tokens_per_dim):\n",
    "    n_token = SPC_TOKENS['<RT_ODG>']+1+n\n",
    "    SPC_TOKENS['<RT_OD'+str(n)+'>'] = n_token\n",
    "last_spc_token = n_token\n",
    "\n",
    "SPC_TOKENS['<RT_NB>']= last_spc_token+1\n",
    "SPC_TOKENS['<RT_NL>']= last_spc_token+2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec34edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = T.Compose([\n",
    "    T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    T.ToTensor(),\n",
    "    T.Resize((args.image_size, args.image_size))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9aef051",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = np.load('../../data/coco_exclude.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e700230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoVisualGenomeDataset(Dataset):\n",
    "    spc_id = -1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        rus_categories_list,\n",
    "        coco_exclude_list,\n",
    "        coco_root: str,\n",
    "        coco_ann_file: str,\n",
    "        vg_annotation_path: str,\n",
    "        vg_images_root: str,\n",
    "        args,\n",
    "        image_transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.coco_root = coco_root\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "        self.image_size = args.image_size\n",
    "        self.bin_size = args.bin_size\n",
    "        self.r_text_seq_length = args.r_text_seq_length\n",
    "        self.l_text_seq_length = args.l_text_seq_length\n",
    "        self.image_tokens_per_dim = args.image_tokens_per_dim\n",
    "        \n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_eos_id = tokenizer.eos_id\n",
    "        self.tokenizer_bos_id = tokenizer.bos_id\n",
    "        self.spc_tokens = SPC_TOKENS\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        text_prompt = '–Ω–∞–π–¥–∏ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ –≤—Å–µ '\n",
    "        \n",
    "        print('LOAD COCO')\n",
    "\n",
    "        self.coco = COCO(coco_ann_file)\n",
    "        self.idxs = list(sorted(self.coco.imgs.keys()))\n",
    "        for ex_img in coco_exclude_list:\n",
    "            self.idxs.remove(ex_img)\n",
    "         \n",
    "        \n",
    "        categories = self.coco.dataset['categories']\n",
    "        cat_idxs = list([cat['id'] for cat in categories])\n",
    "        self.category_encoding_map = {}\n",
    "        for i, cat in enumerate(categories):\n",
    "            self.category_encoding_map[cat['id']] = self.encode_text(text=text_prompt+rus_categories_list[i],\n",
    "                                                                text_seq_length=args.l_text_seq_length)\n",
    "        \n",
    "        \n",
    "        for idx in self.idxs:\n",
    "            anns = self.coco.loadAnns(self.coco.getAnnIds(idx))\n",
    "            anns_df = pd.DataFrame(anns)\n",
    "            if len(anns_df.columns)>0:\n",
    "                cat_id_set = set(anns_df['category_id'])\n",
    "                for cat_id in cat_id_set:\n",
    "                    coco_image = self.coco.loadImgs(idx)[0][\"file_name\"]\n",
    "                    new_sample = {'encoded_left':self.category_encoding_map[cat_id],\n",
    "                                 'bboxes':anns_df[anns_df['category_id']==cat_id]['bbox'].tolist(),\n",
    "                                'image_address':os.path.join(self.coco_root, coco_image)}\n",
    "                    self.data.append(new_sample)\n",
    "                    \n",
    "        print('LOAD VISUAL GENOME')\n",
    "        \n",
    "        self.vg_images_root = vg_images_root\n",
    "        with open(vg_annotation_path, 'rb') as fj:\n",
    "            vg_annotation = json.load(fj)\n",
    "        \n",
    "        for image_name in vg_annotation.keys():\n",
    "            if image_name in os.listdir(self.vg_images_root):\n",
    "                ann_img = vg_annotation[image_name]\n",
    "                for phrase in ann_img.keys():\n",
    "                    new_sample = {'encoded_left':self.encode_text(text=text_prompt+phrase,\n",
    "                                                                text_seq_length=args.l_text_seq_length),\n",
    "                                 'bboxes':[ann_img[phrase][0]],\n",
    "                                'image_address':os.path.join(self.vg_images_root, image_name)}\n",
    "                    self.data.append(new_sample)\n",
    "            \n",
    "    def _load_image(self, image_address) -> Image.Image:\n",
    "        return Image.open(image_address)\n",
    "    \n",
    "    def resize_bbox(self, bbox, img_size):\n",
    "        \n",
    "        bbox_x0 = bbox[0]\n",
    "        bbox_y0 = bbox[1]\n",
    "        bbox_x1 = bbox_x0+bbox[2]\n",
    "        bbox_y1 = bbox_y0+bbox[3]\n",
    "        \n",
    "        img_w = img_size[0]\n",
    "        img_h = img_size[1]\n",
    "        \n",
    "        max_wh = max(img_w,img_h)\n",
    "\n",
    "        resized_x0 = int(bbox_x0*self.image_size/img_w)\n",
    "        resized_y0 = int(bbox_y0*self.image_size/img_h)\n",
    "        resized_x1 = int(bbox_x1*self.image_size/img_w)\n",
    "        resized_y1 = int(bbox_y1*self.image_size/img_h)\n",
    "        return [resized_x0,resized_y0,resized_x1, resized_y1]\n",
    "    \n",
    "    def encode_bbox(self, resized_bbox):\n",
    "        \n",
    "        bbox_x0_bin = resized_bbox[0]//self.bin_size\n",
    "        bbox_y0_bin = resized_bbox[1]//self.bin_size\n",
    "        bbox_x1_bin = min(resized_bbox[2]//self.bin_size, self.image_tokens_per_dim-1)\n",
    "        bbox_y1_bin = min(resized_bbox[3]//self.bin_size, self.image_tokens_per_dim-1)\n",
    "        \n",
    "        new_bbox_tokens = [SPC_TOKENS['<RT_OD'+str(bbox_x0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_x1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y1_bin)+'>']]\n",
    "        \n",
    "        return new_bbox_tokens\n",
    "    \n",
    "    def create_new_box(self):\n",
    "        bbox_x0_bin = random.randint(0, self.args.image_tokens_per_dim-3)\n",
    "        bbox_y0_bin = random.randint(0, self.args.image_tokens_per_dim-3)\n",
    "        bbox_x1_bin = random.randint(bbox_x0_bin, self.args.image_tokens_per_dim-1)\n",
    "        bbox_y1_bin = random.randint(bbox_y0_bin, self.args.image_tokens_per_dim-1)\n",
    "        new_bbox_tokens = [SPC_TOKENS['<RT_OD'+str(bbox_x0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_x1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_NL>']\n",
    "                          ]\n",
    "        return new_bbox_tokens\n",
    "    \n",
    "    def encode_target(self, bboxes, img_size):\n",
    "        encoded_target = [self.tokenizer_bos_id, SPC_TOKENS['<RT_ODG>']]\n",
    "        ground_truth = []\n",
    "        \n",
    "        for i,bbox in enumerate(bboxes):\n",
    "            \n",
    "            resized_bbox = self.resize_bbox(bbox, img_size)\n",
    "            encoded_bbox = self.encode_bbox(resized_bbox)\n",
    "            \n",
    "            ground_truth += resized_bbox\n",
    "            ground_truth += [0]\n",
    "            \n",
    "            for loc_token in encoded_bbox:\n",
    "                encoded_target.append(loc_token)\n",
    "            encoded_target.append(SPC_TOKENS['<RT_NB>'])\n",
    "        for k in range((self.r_text_seq_length-2-len(encoded_target))//5+1):\n",
    "            encoded_target += self.create_new_box()\n",
    "            \n",
    "        encoded_target.append(SPC_TOKENS['<RT_NL>'])\n",
    "        \n",
    "        return torch.Tensor(encoded_target), torch.Tensor(ground_truth)\n",
    "    \n",
    "    def encode_text(self, text, text_seq_length, add_special = True):\n",
    "        tokens = self.tokenizer.tokenizer.encode([text], output_type=yttm.OutputType.ID)[0]\n",
    "        bos = [self.tokenizer.bos_id]\n",
    "        if add_special:\n",
    "            bos.append(self.spc_id)\n",
    "        tokens = bos + tokens + [self.tokenizer.eos_id]\n",
    "        return self.tokenizer.prepare_tokens(tokens, text_seq_length)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = self.data[index]\n",
    "        left_encoded_text = sample['encoded_left']\n",
    "        bboxes = sample['bboxes']\n",
    "        image_address = sample['image_address']\n",
    "        \n",
    "        image = self._load_image(image_address)\n",
    "        img_size = image.size\n",
    "        transformed_image = self.image_transform(image)\n",
    "        \n",
    "        left_special_token = '<LT_ODG>'\n",
    "        right_special_token = '<RT_ODG>'\n",
    "        \n",
    "        left_encoded_text[torch.where(left_encoded_text == self.spc_id)] = self.spc_tokens[left_special_token]\n",
    "        \n",
    "        encoded_target, ground_truth = self.encode_target(bboxes, img_size)\n",
    "\n",
    "        return left_encoded_text, transformed_image, encoded_target, ground_truth    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "038de09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_rus_names = [\"—á–µ–ª–æ–≤–µ–∫\",\"–≤–µ–ª–æ—Å–∏–ø–µ–¥\",\"–∞–≤—Ç–æ–º–æ–±–∏–ª—å\",\"–º–æ—Ç–æ—Ü–∏–∫–ª\",\"—Å–∞–º–æ–ª—ë—Ç\",\"–∞–≤—Ç–æ–±—É—Å\",\"–ø–æ–µ–∑–¥\",\"–≥—Ä—É–∑–æ–≤–∏–∫\",\"–ª–æ–¥–∫–∞\",\n",
    "                 \"—Å–≤–µ—Ç–æ—Ñ–æ—Ä\",\"–≥–∏–¥—Ä–∞–Ω—Ç\",\"–∑–Ω–∞–∫ —Å—Ç–æ–ø\",\"–ø–∞—Ä–∫–æ–≤–æ—á–Ω—ã–π —Å—á–µ—Ç—á–∏–∫\",\"—Å–∫–∞–º–µ–π–∫–∞\",\"–ø—Ç–∏—Ü–∞\",\"–∫–æ—à–∫–∞\",\"—Å–æ–±–∞–∫–∞\",\n",
    "                \"–ª–æ—à–∞–¥—å\",\"–æ–≤—Ü–∞\",\"–∫–æ—Ä–æ–≤–∞\",\"—Å–ª–æ–Ω\",\"–º–µ–¥–≤–µ–¥—å\",\"–∑–µ–±—Ä–∞\",\"–∂–∏—Ä–∞—Ñ\",\"—Ä—é–∫–∑–∞–∫\",\"–∑–æ–Ω—Ç\",\"—Å—É–º–æ—á–∫–∞\",\"–≥–∞–ª—Å—Ç—É–∫\",\n",
    "                \"—á–µ–º–æ–¥–∞–Ω\",\"—Ñ—Ä–∏—Å–±–∏\",\"–ª—ã–∂–∏\",\"—Å–Ω–æ—É–±–æ—Ä–¥\",\"—Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π –º—è—á\",\"–≤–æ–∑–¥—É—à–Ω—ã–π –∑–º–µ–π\",\"–±–µ–π—Å–±–æ–ª—å–Ω–∞—è –±–∏—Ç–∞\",\n",
    "                 \"–±–µ–π—Å–±–æ–ª—å–Ω–∞—è –ø–µ—Ä—á–∞—Ç–∫–∞\",\"—Å–∫–µ–π—Ç–±–æ—Ä–¥\",\"–¥–æ—Å–∫–∞ –¥–ª—è —Å—ë—Ä—Ñ–∏–Ω–≥–∞\",\"—Ç–µ–Ω–Ω–∏—Å–Ω–∞—è —Ä–∞–∫–µ—Ç–∫–∞\",\"–±—É—Ç—ã–ª–∫–∞\",\"–±–æ–∫–∞–ª –¥–ª—è –≤–∏–Ω–∞\",\n",
    "                 \"—á–∞—à–∫–∞\",\"–≤–∏–ª–∫–∞\",\"–Ω–æ–∂\",\"–ª–æ–∂–∫–∞\",\"–º–∏—Å–∫–∞\",\"–±–∞–Ω–∞–Ω\",\"—è–±–ª–æ–∫–æ\",\"—Å—ç–Ω–¥–≤–∏—á\",\"–∞–ø–µ–ª—å—Å–∏–Ω\",\"–±—Ä–æ–∫–∫–æ–ª–∏\",\"–º–æ—Ä–∫–æ–≤—å\",\n",
    "                 \"—Ö–æ–¥-–¥–æ–≥\",\"–ø–∏—Ü—Ü–∞\",\"–ø–æ–Ω—á–∏–∫\",\"—Ç–æ—Ä—Ç\",\"—Å—Ç—É–ª\",\"–¥–∏–≤–∞–Ω\",\"—Ä–∞—Å—Ç–µ–Ω–∏–µ –≤ –≥–æ—Ä—à–∫–µ\",\"–∫—Ä–æ–≤–∞—Ç—å\",\"–æ–±–µ–¥–µ–Ω–Ω—ã–π —Å—Ç–æ–ª\",\"—Ç—É–∞–ª–µ—Ç\",\"—Ç–µ–ª–µ–≤–∏–∑–æ—Ä\",\n",
    "                 \"–Ω–æ—É—Ç–±—É–∫\",\"–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –º—ã—à—å\",\"–ø—É–ª—å—Ç\",\"–∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞\",\"–º–æ–±–∏–ª—å–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω\",\"–º–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–∫–∞\",\"–¥—É—Ö–æ–≤–∫–∞\",\n",
    "                 \"—Ç–æ—Å—Ç–µ—Ä\",\"—Ä–∞–∫–æ–≤–∏–Ω–∞\",\"—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫\",\"–∫–Ω–∏–≥–∞\",\"—á–∞—Å—ã\",\"–≤–∞–∑–∞\",\"–Ω–æ–∂–Ω–∏—Ü—ã\",\"–ø–ª—é—à–µ–≤—ã–π –º–∏—à–∫–∞\",\"—Ñ–µ–Ω\",\"–∑—É–±–Ω–∞—è —â—ë—Ç–∫–∞\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e22bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD COCO\n",
      "loading annotations into memory...\n",
      "Done (t=13.43s)\n",
      "creating index...\n",
      "index created!\n",
      "LOAD VISUAL GENOME\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CocoVisualGenomeDataset(coco_root=r'../../data/train2017',\n",
    "                             coco_ann_file=r'../../data/annotations/instances_train2017.json',\n",
    "                             image_transform=image_transform,\n",
    "                             tokenizer=tokenizer,\n",
    "                             args = args,\n",
    "                             coco_exclude_list=exclude_list,\n",
    "                             rus_categories_list=cat_rus_names,\n",
    "                                      vg_annotation_path=r'../../data/vg_intersection_rus_train.json',\n",
    "                                      vg_images_root = r'../../data/VG_100K'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26dc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "254a9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_right(right_tokens, length=None, scores=True):\n",
    "    boxes = torch.zeros((length, 4))\n",
    "    labels = torch.ones(length)\n",
    "    scores = torch.zeros(length)\n",
    "    print('right_tokens',right_tokens.int())\n",
    "    \n",
    "    print('len(right_tokens)',len(right_tokens),len(right_tokens)//5)\n",
    "    \n",
    "    for k in range(length):\n",
    "        boxes[k,:] = right_tokens[k*5:(k+1)*5-1].unsqueeze(0).cpu()\n",
    "        scores[k] = 1\n",
    "        \n",
    "    boxes -= SPC_TOKENS['<RT_OD0>']*torch.ones((length, 4))\n",
    "    boxes *= args.bin_size\n",
    "\n",
    "    return [{'boxes':boxes, 'labels':labels, 'scores':scores}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e9ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(tokenizer, encoded, ignore_ids):\n",
    "    return tokenizer.tokenizer.decode(encoded.cpu().numpy().tolist(), ignore_ids=ignore_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540514bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_ids = [\n",
    "    tokenizer.eos_id, tokenizer.bos_id, tokenizer.unk_id, tokenizer.pad_id,\n",
    "    -1, *list(SPC_TOKENS.values())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8e5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=args.bs, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8cb154",
   "metadata": {},
   "outputs": [],
   "source": [
    "left, img, tgt = train_dataset.__getitem__(len(train_dataset)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "353fb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False,\n",
    "):\n",
    "    for name, p in model.module.named_parameters():\n",
    "        name = name.lower()\n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            p.requires_grad = not freeze_ff\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f3a9077",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../model/checkpoints/rudolph_object_detection_grounding_neg_sampling_coco_vg_nmsn_02_rudolph_200000.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59ed0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=args.lr, final_div_factor=500, \n",
    "    steps_per_epoch=len(train_dataloader), epochs=args.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a45fa018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, args: Args, train_dataloader, from_checkpoints = False):\n",
    "    \"\"\"\n",
    "      args - arguments for training\n",
    "\n",
    "      train_dataloader - VisualGenomeDataset\n",
    "      \"\"\"\n",
    "    loss_logs = []\n",
    "    encoded_right_text = None\n",
    "    #try:\n",
    "    t_steps = len(train_dataloader)*args.epochs\n",
    "    progress = tqdm(total = t_steps, desc='ü¶åü¶åü¶åfinetuning processü¶åü¶åü¶å')\n",
    "\n",
    "    save_counter = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    if from_checkpoints:\n",
    "        save_counter = max([int(f.split('_')[-1].split('.')[0]) for f in os.listdir(args.save_path) if f.startswith(args.model_name)])\n",
    "        model, optimizer, start_epoch, loss_logs = load_checkpoint(model, optimizer, loss_logs, filename=os.path.join(args.save_path,f\"{args.model_name}state_{save_counter}.pt\"))\n",
    "        start_epoch -= 1\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "        for encoded_left_text, images, encoded_right_text, _ in train_dataloader: #, prompt, image, locs\n",
    "\n",
    "            bs_text = encoded_right_text.shape[0]\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            image_seq_length = args.image_tokens_per_dim ** 2\n",
    "            total_seq_length = args.l_text_seq_length + image_seq_length + args.r_text_seq_length\n",
    "            encoded_images = vae.get_codebook_indices(images, disable_gumbel_softmax=True)\n",
    "\n",
    "            attention_mask_text = get_attention_mask(bs_text, args.l_text_seq_length,\n",
    "                                             args.image_tokens_per_dim, \n",
    "                                             args.r_text_seq_length, args.device)\n",
    "            \n",
    "            input_ids_text = torch.cat((encoded_left_text.to(args.device).int(),\n",
    "                            encoded_images.int(),\n",
    "                            encoded_right_text.to(args.device).int()), dim=1)\n",
    "            loss, loss_values = model.forward(input_ids_text.long(), attention_mask_text, \n",
    "                                                             lt_loss_weight=args.lt_loss_weight,\n",
    "                                                             img_loss_weight=args.img_loss_weight, \n",
    "                                                             rt_loss_weight=args.rt_loss_weight,\n",
    "                                              return_loss=True,\n",
    "                                              category_weight=args.category_weight,\n",
    "                                              fake_category_token = SPC_TOKENS['<RT_NL>'],\n",
    "                                              loc_loss_weight=args.loc_loss_weight,\n",
    "                                              zero_loc_token = SPC_TOKENS['<RT_OD0>'],\n",
    "                                              nms_loss_weight=args.nms_loss_weight,\n",
    "                                                categories_num=args.categories_num,\n",
    "                                                iou_threshold=args.iou_threshold,\n",
    "                                              cat_idx=args.cat_idx,\n",
    "                                              conf_loss_weight=args.conf_loss_weight\n",
    "                                             )\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            save_counter += 1\n",
    "            if save_counter % args.save_every == 0:\n",
    "                print(f'Saving checkpoint here {args.model_name}_rudolph_{save_counter+200000}.pt')\n",
    "                plt.plot(loss_logs)\n",
    "                plt.show()\n",
    "                state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "                         'optimizer': optimizer.state_dict(), 'losslogger': loss_logs}\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(args.save_path,f\"{args.model_name}rudolph_{save_counter+200000}.pt\")\n",
    "                )\n",
    "                torch.save(\n",
    "                    state,\n",
    "                    os.path.join(args.save_path,f\"{args.model_name}state_{save_counter+200000}.pt\")\n",
    "                )\n",
    "\n",
    "            loss_logs+=[loss.item()]\n",
    "            progress.update()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        print(f'Complitly tuned and saved here  {args.model_name}__object_detection_last.pt')\n",
    "        plt.plot(loss_logs)\n",
    "        plt.show()\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(args.save_path,f\"{args.model_name}object_detection_last.pt\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5dde99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = freeze(\n",
    "    model=model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False,\n",
    ")\n",
    "\n",
    "train(model, optimizer, scheduler, args, train_dataloader, from_checkpoints=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
