{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adb8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ..\n",
    "!pip install datasets\n",
    "!pip install pycocotools\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb78a746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rudalle import get_vae\n",
    "from rudalle.utils import seed_everything\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'ru-dolph')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchvision\n",
    "from rudalle.image_prompts import ImagePrompts\n",
    "from rudolph.model import get_rudolph_model, ruDolphModel, FP16Module\n",
    "from rudolph import utils\n",
    "from rudolph.model.utils import get_attention_mask\n",
    "from rudalle import get_tokenizer, get_vae\n",
    "from rudolph.api import ruDolphApi\n",
    "\n",
    "import datasets\n",
    "import random\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from typing import Any, Callable, Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4a6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3d564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f64962",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917d215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_rudolph_model('1.3B', pretrained=True, fp16=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c007d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer --> ready\n",
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n",
      "vae --> ready\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "vae = get_vae(dwt=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd17f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, model, checkpoint_path):\n",
    "        self.device = model.get_param('device')\n",
    "        self.l_text_seq_length = model.get_param('l_text_seq_length')\n",
    "        self.r_text_seq_length = model.get_param('r_text_seq_length')\n",
    "        self.image_tokens_per_dim = model.get_param('image_tokens_per_dim')\n",
    "        self.image_seq_length = model.get_param('image_seq_length')\n",
    "        self.epochs = 1\n",
    "        self.save_path= checkpoint_path\n",
    "        self.model_name = 'rudolph_object_detection_grounding_neg_sampling_coco_vg_'\n",
    "        self.save_every = 10000\n",
    "        self.bs = 1\n",
    "        self.clip = 1.0\n",
    "        self.lr = 2e-5\n",
    "        self.wandb = False\n",
    "        self.lt_loss_weight = 0.01\n",
    "        self.img_loss_weight = 0.01\n",
    "        self.rt_loss_weight = 7\n",
    "        self.loc_loss_weight = 0.05\n",
    "        self.category_weight = 0\n",
    "        self.bin_size = 8\n",
    "        self.ob_seq_len = 5\n",
    "        self.conf_loss_weight=0\n",
    "        self.conf_grad=5\n",
    "        self.cat_idx=4\n",
    "        self.conf_idx=None\n",
    "        self.loc_len=4\n",
    "        self.prob_threshold = 0.15\n",
    "        self.image_size = self.image_tokens_per_dim * self.bin_size\n",
    "        \n",
    "checkpoint_path = '../model/checkpoints/'\n",
    "args = Args(model, checkpoint_path)\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a61611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPC_TOKENS = {\n",
    "    '<LT_UNK>': 16384,\n",
    "    '<RT_UNK>': 16385,\n",
    "    '<LT_T2I>': 16386,\n",
    "    '<LT_I2T>': 16387,\n",
    "    '<LT_T2T>': 16388,\n",
    "    '<RT_I2T>': 16389,\n",
    "    \n",
    "    '<LT_TQA>': 16390,\n",
    "    '<RT_TQA>': 16391,\n",
    "    '<LT_ODG>': 16392,\n",
    "    '<RT_ODG>': 16393,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf34d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(args.image_tokens_per_dim):\n",
    "    n_token = SPC_TOKENS['<RT_ODG>']+1+n\n",
    "    SPC_TOKENS['<RT_OD'+str(n)+'>'] = n_token\n",
    "last_spc_token = n_token\n",
    "\n",
    "SPC_TOKENS['<RT_NB>']= last_spc_token+1\n",
    "SPC_TOKENS['<RT_NL>']= last_spc_token+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dfad713",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = T.Compose([\n",
    "    T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    T.ToTensor(),\n",
    "    T.Resize((args.image_size, args.image_size))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0893c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_val_list = np.load('../../data/coco_val_exclude.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "714970e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoVisualGenomeDataset(Dataset):\n",
    "    spc_id = -1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        rus_categories_list,\n",
    "        coco_exclude_list,\n",
    "        coco_root: str,\n",
    "        coco_ann_file: str,\n",
    "        vg_annotation_path: str,\n",
    "        vg_images_root: str,\n",
    "        args,\n",
    "        add_left_special_token,\n",
    "        coco_len = None,\n",
    "        coco_vg_balance:float=100,\n",
    "        image_transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.coco_root = coco_root\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "        self.image_size = args.image_size\n",
    "        self.bin_size = args.bin_size\n",
    "        self.r_text_seq_length = args.r_text_seq_length\n",
    "        self.l_text_seq_length = args.l_text_seq_length\n",
    "        self.image_tokens_per_dim = args.image_tokens_per_dim\n",
    "        \n",
    "        self.add_left_special_token = add_left_special_token\n",
    "        \n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_eos_id = tokenizer.eos_id\n",
    "        self.tokenizer_bos_id = tokenizer.bos_id\n",
    "        self.spc_tokens = SPC_TOKENS\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        text_prompt = 'найди на картинке все '\n",
    "\n",
    "        self.coco = COCO(coco_ann_file)\n",
    "        self.idxs = list(sorted(self.coco.imgs.keys()))\n",
    "        for ex_img in coco_exclude_list:\n",
    "            self.idxs.remove(ex_img)\n",
    "         \n",
    "        \n",
    "        categories = self.coco.dataset['categories']\n",
    "        cat_idxs = list([cat['id'] for cat in categories])\n",
    "        self.category_encoding_map = {}\n",
    "        for i, cat in enumerate(categories):\n",
    "            self.category_encoding_map[cat['id']] = self.encode_text(text=text_prompt+rus_categories_list[i],\n",
    "                                                                text_seq_length=args.l_text_seq_length,\n",
    "                                                                     add_special = add_left_special_token)\n",
    "        \n",
    "        \n",
    "        coco_counter = 0\n",
    "        for idx in self.idxs:\n",
    "            anns = self.coco.loadAnns(self.coco.getAnnIds(idx))\n",
    "            anns_df = pd.DataFrame(anns)\n",
    "            if len(anns_df.columns)>0:\n",
    "                cat_id_set = set(anns_df['category_id'])\n",
    "                for cat_id in cat_id_set:\n",
    "                    coco_image = self.coco.loadImgs(idx)[0][\"file_name\"]\n",
    "                    new_sample = {'encoded_left':self.category_encoding_map[cat_id],\n",
    "                                 'bboxes':anns_df[anns_df['category_id']==cat_id]['bbox'].tolist(),\n",
    "                                'image_address':os.path.join(self.coco_root, coco_image)}\n",
    "                    self.data.append(new_sample)\n",
    "                    coco_counter += 1\n",
    "            if coco_len is not None and coco_counter>coco_len:\n",
    "                break\n",
    "        \n",
    "        self.vg_images_root = vg_images_root\n",
    "        with open(vg_annotation_path, 'rb') as fj:\n",
    "            vg_annotation = json.load(fj)\n",
    "        \n",
    "        vg_counter = 0\n",
    "        for image_name in vg_annotation.keys():\n",
    "            if vg_counter < coco_counter * coco_vg_balance:\n",
    "                if image_name in os.listdir(self.vg_images_root):\n",
    "                    ann_img = vg_annotation[image_name]\n",
    "                    for phrase in ann_img.keys():\n",
    "                        new_sample = {'encoded_left':self.encode_text(text=text_prompt+phrase,\n",
    "                                                                    text_seq_length=args.l_text_seq_length,\n",
    "                                                                     add_special = add_left_special_token),\n",
    "                                     'bboxes':[ann_img[phrase][0]],\n",
    "                                    'image_address':os.path.join(self.vg_images_root, image_name)}\n",
    "                        self.data.append(new_sample)\n",
    "                        vg_counter += 1\n",
    "            \n",
    "    def _load_image(self, image_address) -> Image.Image:\n",
    "        return Image.open(image_address)\n",
    "    \n",
    "    def resize_bbox(self, bbox, img_size):\n",
    "        \n",
    "        bbox_x0 = bbox[0]\n",
    "        bbox_y0 = bbox[1]\n",
    "        bbox_x1 = bbox_x0+bbox[2]\n",
    "        bbox_y1 = bbox_y0+bbox[3]\n",
    "        \n",
    "        img_w = img_size[0]\n",
    "        img_h = img_size[1]\n",
    "        \n",
    "        max_wh = max(img_w,img_h)\n",
    "\n",
    "        resized_x0 = int(bbox_x0*self.image_size/img_w)\n",
    "        resized_y0 = int(bbox_y0*self.image_size/img_h)\n",
    "        resized_x1 = int(bbox_x1*self.image_size/img_w)\n",
    "        resized_y1 = int(bbox_y1*self.image_size/img_h)\n",
    "        return [resized_x0,resized_y0,resized_x1, resized_y1]\n",
    "    \n",
    "    def encode_bbox(self, resized_bbox):\n",
    "        \n",
    "        bbox_x0_bin = resized_bbox[0]//self.bin_size\n",
    "        bbox_y0_bin = resized_bbox[1]//self.bin_size\n",
    "        bbox_x1_bin = min(resized_bbox[2]//self.bin_size, self.image_tokens_per_dim-1)\n",
    "        bbox_y1_bin = min(resized_bbox[3]//self.bin_size, self.image_tokens_per_dim-1)\n",
    "        \n",
    "        new_bbox_tokens = [SPC_TOKENS['<RT_OD'+str(bbox_x0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_x1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y1_bin)+'>']]\n",
    "        \n",
    "        return new_bbox_tokens\n",
    "    \n",
    "    def create_new_box(self):\n",
    "        bbox_x0_bin = random.randint(0, self.args.image_tokens_per_dim-3)\n",
    "        bbox_y0_bin = random.randint(0, self.args.image_tokens_per_dim-3)\n",
    "        bbox_x1_bin = random.randint(bbox_x0_bin, self.args.image_tokens_per_dim-1)\n",
    "        bbox_y1_bin = random.randint(bbox_y0_bin, self.args.image_tokens_per_dim-1)\n",
    "        new_bbox_tokens = [SPC_TOKENS['<RT_OD'+str(bbox_x0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_x1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_NL>']\n",
    "                          ]\n",
    "        return new_bbox_tokens\n",
    "    \n",
    "    def encode_target(self, bboxes, img_size):\n",
    "        encoded_target = [self.tokenizer_bos_id, SPC_TOKENS['<RT_ODG>']]\n",
    "        ground_truth = []\n",
    "        \n",
    "        for i,bbox in enumerate(bboxes):\n",
    "            \n",
    "            resized_bbox = self.resize_bbox(bbox, img_size)\n",
    "            encoded_bbox = self.encode_bbox(resized_bbox)\n",
    "            \n",
    "            ground_truth += resized_bbox\n",
    "            ground_truth += [0]\n",
    "            \n",
    "            for loc_token in encoded_bbox:\n",
    "                encoded_target.append(loc_token)\n",
    "            encoded_target.append(SPC_TOKENS['<RT_NB>'])\n",
    "        for k in range((self.r_text_seq_length-2-len(encoded_target))//5+1):\n",
    "            encoded_target += self.create_new_box()\n",
    "            \n",
    "        encoded_target.append(SPC_TOKENS['<RT_NL>'])\n",
    "        \n",
    "        return torch.Tensor(encoded_target), torch.Tensor(ground_truth)\n",
    "    \n",
    "    def encode_text(self, text, text_seq_length, add_special = True):\n",
    "        tokens = self.tokenizer.tokenizer.encode([text], output_type=yttm.OutputType.ID)[0]\n",
    "        bos = [self.tokenizer.bos_id]\n",
    "        if add_special:\n",
    "            bos.append(self.spc_id)\n",
    "        tokens = bos + tokens + [self.tokenizer.eos_id]\n",
    "        return self.tokenizer.prepare_tokens(tokens, text_seq_length)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = self.data[index]\n",
    "        left_encoded_text = sample['encoded_left']\n",
    "        bboxes = sample['bboxes']\n",
    "        image_address = sample['image_address']\n",
    "        \n",
    "        image = self._load_image(image_address)\n",
    "        img_size = image.size\n",
    "        transformed_image = self.image_transform(image)\n",
    "        \n",
    "        left_special_token = '<LT_ODG>'\n",
    "        right_special_token = '<RT_ODG>'\n",
    "        \n",
    "        left_encoded_text[torch.where(left_encoded_text == self.spc_id)] = self.spc_tokens[left_special_token]\n",
    "        \n",
    "        encoded_target, ground_truth = self.encode_target(bboxes, img_size)\n",
    "\n",
    "        return left_encoded_text, transformed_image, encoded_target, ground_truth\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aaf4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_rus_names = [\"человек\",\"велосипед\",\"автомобиль\",\"мотоцикл\",\"самолёт\",\"автобус\",\"поезд\",\"грузовик\",\"лодка\",\n",
    "                 \"светофор\",\"гидрант\",\"знак стоп\",\"парковочный счетчик\",\"скамейка\",\"птица\",\"кошка\",\"собака\",\n",
    "                \"лошадь\",\"овца\",\"корова\",\"слон\",\"медведь\",\"зебра\",\"жираф\",\"рюкзак\",\"зонт\",\"сумочка\",\"галстук\",\n",
    "                \"чемодан\",\"фрисби\",\"лыжи\",\"сноуборд\",\"спортивный мяч\",\"воздушный змей\",\"бейсбольная бита\",\n",
    "                 \"бейсбольная перчатка\",\"скейтборд\",\"доска для сёрфинга\",\"теннисная ракетка\",\"бутылка\",\"бокал для вина\",\n",
    "                 \"чашка\",\"вилка\",\"нож\",\"ложка\",\"миска\",\"банан\",\"яблоко\",\"сэндвич\",\"апельсин\",\"брокколи\",\"морковь\",\n",
    "                 \"ход-дог\",\"пицца\",\"пончик\",\"торт\",\"стул\",\"диван\",\"растение в горшке\",\"кровать\",\"обеденный стол\",\"туалет\",\"телевизор\",\n",
    "                 \"ноутбук\",\"компьютерная мышь\",\"пульт\",\"клавиатура\",\"мобильный телефон\",\"микроволновка\",\"духовка\",\n",
    "                 \"тостер\",\"раковина\",\"холодильник\",\"книга\",\"часы\",\"ваза\",\"ножницы\",\"плюшевый мишка\",\"фен\",\"зубная щётка\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "255ff699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD COCO\n",
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n",
      "LOAD VISUAL GENOME\n"
     ]
    }
   ],
   "source": [
    "val_dataset = CocoVisualGenomeDataset(coco_root=r'../../data/val2017',\n",
    "                             coco_ann_file=r'../../data/annotations/instances_val2017.json',\n",
    "                                      coco_len = 3000,\n",
    "                                coco_vg_balance=5000/30000,\n",
    "                             image_transform=image_transform,\n",
    "                             tokenizer=tokenizer,\n",
    "                             args = args,\n",
    "                             coco_exclude_list=exclude_val_list,\n",
    "                             rus_categories_list=cat_rus_names,\n",
    "                                      vg_annotation_path=r'../../data/vg_intersection_rus_val_4.json',\n",
    "                                      vg_images_root = r'../../data/VG_100K_2',\n",
    "                                      add_left_special_token = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11c0fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(tokenizer, encoded, ignore_ids):\n",
    "    return tokenizer.tokenizer.decode(encoded.cpu().numpy().tolist(), ignore_ids=ignore_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb07304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_ids = [\n",
    "    tokenizer.eos_id, tokenizer.bos_id, tokenizer.unk_id, tokenizer.pad_id,\n",
    "    -1, *list(SPC_TOKENS.values())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a41b0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../model/checkpoints/rudolph_object_detection_grounding_neg_sampling_coco_vg_nmsn_rudolph_300000.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "463f7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_to_coords(token:int) -> float:\n",
    "    num_token = token - SPC_TOKENS['<RT_OD0>']\n",
    "    return args.bin_size*num_token\n",
    "\n",
    "def calc_area(bbox):\n",
    "    w = bbox[2]-bbox[0]\n",
    "    h = bbox[3]-bbox[1]\n",
    "    return w*h\n",
    "\n",
    "def tokens_to_bbox(tokens):\n",
    "    bbox = torch.zeros(1,4)\n",
    "    bbox[0,0] = loc_to_coords(tokens[0])\n",
    "    bbox[0,1] = loc_to_coords(tokens[1])\n",
    "    bbox[0,2] = loc_to_coords(tokens[2])\n",
    "    bbox[0,3] = loc_to_coords(tokens[3])\n",
    "    return bbox\n",
    "\n",
    "def tokens_iou(pred_tokens, ground_truth_bbox):\n",
    "    pred_bbox = tokens_to_bbox(pred_tokens)\n",
    "    print(pred_bbox,ground_truth_bbox)\n",
    "    iou = ops.box_iou(ground_truth_bbox, pred_bbox)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f387b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ruDolphPhraseGroundingApi(ruDolphApi):\n",
    "    def __init__(self, model, tokenizer, vae, spc_tokens=None, quite=False, *, bs=24, q=0.5, txt_top_k=64,\n",
    "                 img_top_k=768, txt_top_p=0.8, img_top_p=0.99, txt_temperature=0.9, img_temperature=1.0):\n",
    "        \n",
    "        super().__init__(model = model, tokenizer = tokenizer, vae = vae, spc_tokens = spc_tokens, quite = quite, bs = bs, q = q, txt_top_k = txt_top_k,\n",
    "                 img_top_k = img_top_k, txt_top_p = txt_top_p, img_top_p = img_top_p, txt_temperature = txt_temperature, img_temperature = img_temperature)\n",
    "    \n",
    "    def generate_text_answers(self, image_tokens, left_text, vocab_size, top_k, top_p, temperature=1.0,\n",
    "                              use_cache=True, template='', allowed_token_ids=None, special_token='',obj_det_seq_len=5):\n",
    "        '''\n",
    "            Generate right text tokens.\n",
    "        '''\n",
    "        self.model.eval()\n",
    "        torch.cuda.empty_cache()\n",
    "        generated_tokens = []\n",
    "        chunk_bs = left_text.shape[0]\n",
    "        \n",
    "        template = template.lower().strip()\n",
    "        template_encoded = self.encode_text(template, text_seq_length=self.r_text_seq_length)\n",
    "        \n",
    "        template_size = (template_encoded != 0).sum() - 1  # eos\n",
    "        \n",
    "        template_encoded = template_encoded[:template_size]\n",
    "        template_encoded[torch.where(template_encoded == self.spc_id)] = self.spc_tokens[special_token]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            attention_mask = self.get_attention_mask(chunk_bs)\n",
    "\n",
    "            out = torch.cat((\n",
    "                left_text.to(self.device),\n",
    "                image_tokens,\n",
    "                template_encoded.repeat(chunk_bs, 1).to(self.device),\n",
    "            ), dim=1)\n",
    "\n",
    "            cache = None\n",
    "            iter_range = range(\n",
    "                self.l_text_seq_length + self.image_seq_length + template_size, \n",
    "                self.l_text_seq_length + self.image_seq_length + self.r_text_seq_length\n",
    "            )\n",
    "            iter_start = self.l_text_seq_length + self.image_seq_length + template_size-1\n",
    "            \n",
    "            if not self.quite:\n",
    "                iter_range = tqdm(iter_range)\n",
    "                \n",
    "            scores = torch.zeros(self.r_text_seq_length-template_size)\n",
    "                \n",
    "            for it in iter_range:  \n",
    "                logits, cache = self.model(out, attention_mask, cache=cache, use_cache=use_cache, return_loss=False)\n",
    "                \n",
    "                logits = logits[:, -1, :self.vocab_size]\n",
    "            \n",
    "                if allowed_token_ids:\n",
    "                    logits = logits[:, allowed_token_ids]\n",
    "            \n",
    "                logits /= temperature\n",
    "                filtered_logits = transformers.top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "                probs = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "                \n",
    "                sample = torch.multinomial(probs, 1)\n",
    "                if allowed_token_ids:\n",
    "                        sample = torch.tensor(allowed_token_ids).to(self.device)[sample]\n",
    "                \n",
    "                scores[it-iter_start-1] = torch.max(probs, dim=1)[0]\n",
    "            \n",
    "                indexes = torch.where(sample >= self.vocab_size - self.l_text_seq_length)\n",
    "                sample[indexes] = 3\n",
    "                out = torch.cat((out, sample), dim=-1)\n",
    "\n",
    "            generated_tokens.append(out[:, -self.r_text_seq_length:])\n",
    "\n",
    "        tokens = torch.cat(generated_tokens)[:,:]\n",
    "\n",
    "        return tokens, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16661327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_right(right_tokens, length=None, scores=True, obj_det_seq_len=5, cat_idx=4):\n",
    "    boxes = torch.zeros((length, 4))\n",
    "    labels = torch.ones(length)\n",
    "    scores = torch.zeros(length)\n",
    "    \n",
    "    real_length = len(right_tokens)//5\n",
    "    \n",
    "    for k in range(real_length):\n",
    "        boxes[k,:] = right_tokens[k*obj_det_seq_len:obj_det_seq_len*5+cat_idx].unsqueeze(0).cpu()\n",
    "        scores[k] = 1\n",
    "        labels[k] = 0\n",
    "        \n",
    "    boxes[:real_length] -= SPC_TOKENS['<RT_OD0>']*torch.ones((real_length, 4))\n",
    "    boxes *= args.bin_size\n",
    "\n",
    "    return [{'boxes':boxes, 'labels':labels, 'scores':scores}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40d5e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_right_prob(right_tokens, probs, prob_threshold=0, obj_det_seq_len=5, length=None, scores=True):\n",
    "    bboxes = []\n",
    "    boxes = torch.empty(0,4)\n",
    "\n",
    "    real_length = len(right_tokens)//obj_det_seq_len\n",
    "    \n",
    "    for k in range(real_length):\n",
    "        prob = probs[k*obj_det_seq_len]*probs[k*obj_det_seq_len+1]*probs[k*obj_det_seq_len+2]*probs[k*obj_det_seq_len+3]*probs[k*obj_det_seq_len+4]\n",
    "        if prob>prob_threshold:\n",
    "            bboxes.append(right_tokens[k*obj_det_seq_len:(k+1)*obj_det_seq_len-1].unsqueeze(0).cpu().float())\n",
    "            \n",
    "    if len(bboxes)>0:\n",
    "        boxes = torch.vstack(bboxes)\n",
    "        \n",
    "    if len(boxes)<length:\n",
    "        boxes = torch.vstack([boxes, torch.zeros(length-len(boxes),4)])\n",
    "    labels = torch.ones(boxes.size(0))\n",
    "    scores = torch.zeros(boxes.size(0))\n",
    "    labels[:len(bboxes)] = torch.zeros(len(bboxes))  \n",
    "    scores[:len(bboxes)] = torch.ones(len(bboxes))\n",
    "        \n",
    "    boxes[:len(bboxes)] -= SPC_TOKENS['<RT_OD0>']*torch.ones((len(bboxes), 4)).float()\n",
    "    boxes *= args.bin_size\n",
    "\n",
    "    return [{'boxes':boxes, 'labels':labels, 'scores':scores}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b013fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_empty_samples(res, new_length):\n",
    "    res_size = res['boxes'].size(0)\n",
    "    \n",
    "    boxes = torch.zeros((new_length, 4))\n",
    "    labels = (SPC_TOKENS['<RT_NL>']-SPC_TOKENS['<RT_NB>'])*torch.ones(new_length)\n",
    "    scores = torch.zeros(new_length)\n",
    "    \n",
    "    boxes[:res_size,:] = res['boxes']\n",
    "    labels[:res_size] = res['labels']\n",
    "    scores[:res_size] = res['scores']\n",
    "    \n",
    "    return [{'boxes':boxes, 'labels':labels, 'scores':scores}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "836c704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ground_truth(ground_truth, length=None, scores=True, obj_det_seq_len=5, cat_idx=4):\n",
    "    boxes = torch.zeros((length, 4))\n",
    "    labels = (SPC_TOKENS['<RT_NL>']-SPC_TOKENS['<RT_NB>'])*torch.ones(length)\n",
    "    scores = torch.zeros(length)\n",
    "\n",
    "    for k in range(len(ground_truth)//5):\n",
    "        labels[k] = ground_truth[k*obj_det_seq_len+cat_idx].unsqueeze(0).cpu()\n",
    "        boxes[k,:] = ground_truth[k*obj_det_seq_len:obj_det_seq_len*5+cat_idx].unsqueeze(0).cpu()\n",
    "        scores[k] = 1\n",
    "\n",
    "    return [{'boxes':boxes, 'labels':labels, 'scores':scores}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b4bc4",
   "metadata": {},
   "source": [
    "# пример детекции на валидационном сете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bfc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=11\n",
    "\n",
    "encoded_left_text, images, encoded_right_text, _ = val_dataset.__getitem__(n)\n",
    "\n",
    "encoded_left_text = encoded_left_text.unsqueeze(0)\n",
    "images = images.unsqueeze(0)\n",
    "encoded_right_text = encoded_right_text.unsqueeze(0)\n",
    "\n",
    "print(decode_text(tokenizer, encoded_left_text, ignore_ids))\n",
    "    \n",
    "bs_text = encoded_left_text.shape[0]\n",
    "\n",
    "images = images.to(device)\n",
    "\n",
    "image_seq_length = args.image_tokens_per_dim ** 2\n",
    "total_seq_length = args.l_text_seq_length + image_seq_length + args.r_text_seq_length\n",
    "encoded_images = vae.get_codebook_indices(images, disable_gumbel_softmax=True)\n",
    "\n",
    "## Генерируем правые текстовые токены\n",
    "pred_tokens, probs = api.generate_text_answers(encoded_images, encoded_left_text.type(torch.LongTensor), api.vocab_size, \n",
    "                            top_k=32, top_p=0.8, temperature=1.0, template = '', \n",
    "                            allowed_token_ids =allowed_token_ids, special_token='<RT_ODG>')\n",
    "fake_cat_token = SPC_TOKENS['<RT_NL>']\n",
    "\n",
    "target_fake_cat_token_idxs = torch.where(encoded_right_text[0,:]==fake_cat_token)[0]\n",
    "if len(target_fake_cat_token_idxs)>0:\n",
    "    target_end_idx = min(target_fake_cat_token_idxs)-4\n",
    "else:\n",
    "    target_end_idx=args.r_text_seq_length\n",
    "\n",
    "if len(torch.where(pred_tokens[0,:]==fake_cat_token)[0])>0:\n",
    "    pred_end_idx = min(torch.where(pred_tokens[0,:]==fake_cat_token)[0].cpu().numpy())-4\n",
    "else:\n",
    "    pred_end_idx=args.r_text_seq_length\n",
    "\n",
    "target_len = (target_end_idx-2)//args.ob_seq_len\n",
    "pred_len = (pred_end_idx-2)//args.ob_seq_len\n",
    "\n",
    "pred_a = decode_right(pred_tokens[0,2:pred_end_idx], length=max(target_len,pred_len))\n",
    "pred =  decode_right_prob(pred_tokens[0,2:pred_end_idx], probs=probs, prob_threshold=0.15,\n",
    "                           obj_det_seq_len=args.ob_seq_len, length=target_len)\n",
    "\n",
    "target = decode_right(encoded_right_text[0,2:target_end_idx], length=max(target_len,pred_len))\n",
    "\n",
    "img = images.squeeze(0).cpu()\n",
    "\n",
    "img *= 255\n",
    "img = img.to(torch.uint8)\n",
    "\n",
    "real_len = len(torch.where(target[0]['labels']!=1)[0])\n",
    "\n",
    "colors = [(255,0,0)]*real_len \n",
    "img_with_bboxes = draw_bounding_boxes(\n",
    "    img, boxes=target[0][\"boxes\"][:real_len],colors=colors, width=2)\n",
    "\n",
    "pred_len = len(torch.where(pred[0]['labels']!=1)[0])\n",
    "pred_colors = [(0,0,255)]*pred_len \n",
    "img_with_bboxes = draw_bounding_boxes(\n",
    "    img_with_bboxes, boxes=pred[0][\"boxes\"][:pred_len],colors=pred_colors, width=2)\n",
    "\n",
    "img_with_bboxes = TF.to_pil_image(img_with_bboxes.detach())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "plt.imshow(img_with_bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f097b0",
   "metadata": {},
   "source": [
    "# детекция по пользовательскому запросу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fa390b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ruDolphPhraseGroundingApi(model, tokenizer, vae, spc_tokens=SPC_TOKENS)\n",
    "allowed_token_ids = list(SPC_TOKENS.values())+[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=170\n",
    "\n",
    "encoded_left_text, images, encoded_right_text, _ = val_dataset.__getitem__(n)\n",
    "\n",
    "text_prompt = 'найди на картинке всех '\n",
    "text = 'кожаных мешков'\n",
    "encoded_left_text = val_dataset.encode_text(text=text_prompt+text,text_seq_length=args.l_text_seq_length, add_special = True)\n",
    "encoded_left_text[torch.where(encoded_left_text == val_dataset.spc_id)] = val_dataset.spc_tokens['<LT_ODG>']\n",
    "\n",
    "encoded_left_text = encoded_left_text.unsqueeze(0)\n",
    "images = images.unsqueeze(0)\n",
    "encoded_right_text = encoded_right_text.unsqueeze(0)\n",
    "\n",
    "print(decode_text(tokenizer, encoded_left_text, ignore_ids))\n",
    "    \n",
    "bs_text = encoded_left_text.shape[0]\n",
    "\n",
    "images = images.to(device)\n",
    "\n",
    "image_seq_length = args.image_tokens_per_dim ** 2\n",
    "total_seq_length = args.l_text_seq_length + image_seq_length + args.r_text_seq_length\n",
    "encoded_images = vae.get_codebook_indices(images, disable_gumbel_softmax=True)\n",
    "\n",
    "## Генерируем правые текстовые токены\n",
    "pred_tokens, probs = api.generate_text_answers(encoded_images, encoded_left_text.type(torch.LongTensor), api.vocab_size, \n",
    "                            top_k=32, top_p=0.8, temperature=1.0, template = '', \n",
    "                            allowed_token_ids =allowed_token_ids, special_token='<RT_ODG>')\n",
    "fake_cat_token = SPC_TOKENS['<RT_NL>']\n",
    "\n",
    "target_fake_cat_token_idxs = torch.where(encoded_right_text[0,:]==fake_cat_token)[0]\n",
    "if len(target_fake_cat_token_idxs)>0:\n",
    "    target_end_idx = min(target_fake_cat_token_idxs)-4\n",
    "else:\n",
    "    target_end_idx=args.r_text_seq_length\n",
    "\n",
    "if len(torch.where(pred_tokens[0,:]==fake_cat_token)[0])>0:\n",
    "    pred_end_idx = min(torch.where(pred_tokens[0,:]==fake_cat_token)[0].cpu().numpy())-4\n",
    "else:\n",
    "    pred_end_idx=args.r_text_seq_length\n",
    "\n",
    "target_len = (target_end_idx-2)//args.ob_seq_len\n",
    "pred_len = (pred_end_idx-2)//args.ob_seq_len\n",
    "\n",
    "pred_a = decode_right(pred_tokens[0,2:pred_end_idx], length=max(target_len,pred_len))\n",
    "pred =  decode_right_prob(pred_tokens[0,2:pred_end_idx], probs=probs, prob_threshold=0.,\n",
    "                           obj_det_seq_len=args.ob_seq_len, length=target_len)\n",
    "\n",
    "target = decode_right(encoded_right_text[0,2:target_end_idx], length=max(target_len,pred_len))\n",
    "\n",
    "img = images.squeeze(0).cpu()\n",
    "\n",
    "img *= 255\n",
    "img = img.to(torch.uint8)\n",
    "\n",
    "real_len = len(torch.where(target[0]['labels']!=1)[0])\n",
    "\n",
    "pred_len = len(torch.where(pred[0]['labels']!=1)[0])\n",
    "pred_colors = [(0,0,255)]*pred_len \n",
    "img_with_bboxes = draw_bounding_boxes(\n",
    "    img, boxes=pred[0][\"boxes\"][:pred_len],colors=pred_colors, width=2)\n",
    "\n",
    "img_with_bboxes = TF.to_pil_image(img_with_bboxes.detach())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "plt.imshow(img_with_bboxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccaec60",
   "metadata": {},
   "source": [
    "# метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ruDolphPhraseGroundingApi(model, tokenizer, vae, spc_tokens=SPC_TOKENS)\n",
    "metric_rt = MeanAveragePrecision()\n",
    "metric_gt = MeanAveragePrecision()\n",
    "pred_lens = []\n",
    "tgt_lens = []\n",
    "preds = []\n",
    "tgts = []\n",
    "gts = []\n",
    "allowed_token_ids = list(SPC_TOKENS.values())+[0,3]\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for i,batch in enumerate(val_dataloader):\n",
    "    encoded_left_text, images, encoded_right_text, ground_truth = batch\n",
    "    \n",
    "    bs_text = encoded_left_text.shape[0]\n",
    "\n",
    "    images = images.to(device)\n",
    "\n",
    "    image_seq_length = args.image_tokens_per_dim ** 2\n",
    "    total_seq_length = args.l_text_seq_length + image_seq_length + args.r_text_seq_length\n",
    "    encoded_images = vae.get_codebook_indices(images, disable_gumbel_softmax=True)\n",
    "\n",
    "    ## Генерируем правые текстовые токены\n",
    "    pred_tokens, probs = api.generate_text_answers(encoded_images, encoded_left_text.type(torch.LongTensor), api.vocab_size, \n",
    "                                top_k=32, top_p=0.8, temperature=1.0, template = '', \n",
    "                                allowed_token_ids =allowed_token_ids, special_token='<RT_ODG>')\n",
    "    fake_cat_token = SPC_TOKENS['<RT_NL>']\n",
    "    \n",
    "    target_fake_cat_token_idxs = torch.where(encoded_right_text[0,:]==fake_cat_token)[0]\n",
    "    if len(target_fake_cat_token_idxs)>0:\n",
    "        target_end_idx = min(target_fake_cat_token_idxs)-4\n",
    "    else:\n",
    "        target_end_idx=args.r_text_seq_length\n",
    "    \n",
    "    if len(torch.where(pred_tokens[0,:]==fake_cat_token)[0])>0:\n",
    "        pred_end_idx = min(torch.where(pred_tokens[0,:]==fake_cat_token)[0].cpu().numpy())-4\n",
    "    else:\n",
    "        pred_end_idx=args.r_text_seq_length\n",
    "        \n",
    "    cat_idxs = [1 + (k+1)*5 for k in range(args.r_text_seq_length//5)]\n",
    "    min_cat_idxs = torch.where(pred_tokens[0,cat_idxs]<SPC_TOKENS['<RT_NB>'])[0]\n",
    "    if len(min_cat_idxs)>0:\n",
    "        pred_end_idx = min(cat_idxs[min(min_cat_idxs).int()], pred_end_idx)\n",
    "    \n",
    "        \n",
    "    target_len = (target_end_idx-2)//args.ob_seq_len\n",
    "    pred_len = (pred_end_idx-2)//args.ob_seq_len\n",
    "    gt_len = len(ground_truth[0])//args.ob_seq_len\n",
    "    \n",
    "    gt = decode_ground_truth(ground_truth[0], length=gt_len)\n",
    "    pred = decode_right_prob(pred_tokens[0,2:pred_end_idx], probs=probs, prob_threshold=args.prob_threshold,\n",
    "                           obj_det_seq_len=args.ob_seq_len, length=target_len)\n",
    "    pred_len = pred[0]['boxes'].size(0)\n",
    "    \n",
    "    if pred[0]['boxes'].size(0) > gt[0]['boxes'].size(0):\n",
    "        gt = add_empty_samples(gt[0], pred[0]['boxes'].size(0))\n",
    "    elif pred[0]['boxes'].size(0) < gt[0]['boxes'].size(0):\n",
    "        pred = add_empty_samples(pred[0], gt[0]['boxes'].size(0))        \n",
    "    target = decode_right(encoded_right_text[0,2:target_end_idx], length=max(target_len,pred[0]['boxes'].size(0)))\n",
    "    \n",
    "    metric_gt.update(pred, gt)\n",
    "    metric_rt.update(pred, target)\n",
    "    \n",
    "    preds.append(pred)\n",
    "    tgts.append(target)\n",
    "    gts.append(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7b46165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox2coco(bbox):\n",
    "    return [float(bbox[0]), float(bbox[1]), float(bbox[2]-bbox[0]), float(bbox[3]-bbox[1])]\n",
    "\n",
    "def create_coco(samples):\n",
    "    res_coco = {'info': {'description': 'rask',\n",
    "      'url': 'http://none',\n",
    "      'version': '1.0',\n",
    "      'year': 2022,\n",
    "      'contributor': 'rask',\n",
    "      'date_created': '2022/12/26'},\n",
    "     'licenses':[]}\n",
    "    anns = []\n",
    "    count = 0\n",
    "    images = []\n",
    "    for idx, sample in enumerate(samples):\n",
    "        anns_from_sample = sample2ann_res(sample[0],idx)\n",
    "        image = {'id':idx, 'file_name':'', 'height':256, 'width':256}\n",
    "        images.append(image)\n",
    "        for ann_s in anns_from_sample:\n",
    "            ann = {}\n",
    "            ann['id'] = count\n",
    "            ann['image_id'] = ann_s[\"image_id\"]\n",
    "            ann['iscrowd'] = 0\n",
    "            ann[\"category_id\"] = ann_s[\"category_id\"]\n",
    "            ann['bbox'] = ann_s['bbox']\n",
    "            ann['area'] = ann_s['bbox'][2]*ann_s['bbox'][3]\n",
    "            x1, x2 = ann_s['bbox'][0], ann_s['bbox'][0]+ann_s['bbox'][2]\n",
    "            y1, y2 = ann_s['bbox'][1], ann_s['bbox'][1]+ann_s['bbox'][3]\n",
    "            ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n",
    "            anns.append(ann)\n",
    "            count += 1\n",
    "    res_coco['images'] = images\n",
    "    res_coco['annotations'] = anns\n",
    "    res_coco['categories'] = [{\"id\": 0, \"name\": \"real object\", \"supercategory\": \"real object\"},\n",
    "                              {\"id\": 1, \"name\": \"fake object\", \"supercategory\": \"fake object\"}]\n",
    "    return res_coco\n",
    "\n",
    "def sample2ann_res(sample, image_id):\n",
    "    fake_idxs = torch.where(sample['boxes']==0)[0]\n",
    "    lst_idx = 1\n",
    "    if len(fake_idxs)>0:\n",
    "        lst_idx = min(fake_idxs)\n",
    "    res = []\n",
    "    for k in range(lst_idx):\n",
    "        coco_sample = {}\n",
    "        coco_sample['id'] = k\n",
    "        coco_sample['bbox'] = bbox2coco(sample['boxes'][k,:])\n",
    "        coco_sample['score'] = int(sample['scores'][k])*100\n",
    "        coco_sample[\"category_id\"] = int(sample['labels'][k])\n",
    "        coco_sample[\"image_id\"] = image_id\n",
    "        res.append(coco_sample)\n",
    "    return res\n",
    "\n",
    "def create_results(samples):\n",
    "    results = []\n",
    "    r_count = 0\n",
    "    for idx, sample in enumerate(samples):\n",
    "        res = sample2ann_res(sample[0], image_id=idx)\n",
    "        for r in res:\n",
    "            r['id'] = r_count\n",
    "            results.append(r)\n",
    "            r_count += 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_s =  create_coco(tgts)\n",
    "g_s =  create_coco(gts)\n",
    "p_s = create_results(preds)\n",
    "\n",
    "json_object = json.dumps(p_s, indent=4)\n",
    "with open(r\"../../data/m16_vg_coco_valid_predictions.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "    \n",
    "json_object = json.dumps(t_s, indent=4)\n",
    "with open(r\"../../data/m16_vg_coco_valid_targets.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "    \n",
    "json_object = json.dumps(g_s, indent=4)\n",
    "with open(r\"../../data/m16_vg_coco_valid_ground_truth.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a07454",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoGt = COCO(r\"../../data/m16_vg_coco_valid_ground_truth.json\")\n",
    "cocoDt = cocoGt.loadRes(r\"../../data/m16_vg_coco_valid_predictions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb53bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoEval = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "060ac190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.122\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.039\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.065\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.201\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.110\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.112\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.112\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.266\n"
     ]
    }
   ],
   "source": [
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
