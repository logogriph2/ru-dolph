{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acf8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ..\n",
    "!pip install datasets\n",
    "!pip install pycocotools\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d018924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rudalle import get_vae\n",
    "from rudalle.utils import seed_everything\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'ru-dolph')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchvision\n",
    "from rudalle.image_prompts import ImagePrompts\n",
    "from rudolph.model import get_rudolph_model, ruDolphModel, FP16Module\n",
    "from rudolph import utils\n",
    "from rudolph.model.utils import get_attention_mask\n",
    "from rudalle import get_tokenizer, get_vae\n",
    "from rudolph.api import ruDolphApi\n",
    "\n",
    "import datasets\n",
    "import random\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Any, Callable, Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57a8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61ce511",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662305bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(45)\n",
    "np.random.seed(45)\n",
    "random.seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7088254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e36e522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW VERSION!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-rudolph-obj-detection-0/lib/python3.7/site-packages/huggingface_hub/file_download.py:632: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = get_rudolph_model('1.3B', pretrained=True, fp16=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65a56e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer --> ready\n",
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n",
      "vae --> ready\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "vae = get_vae(dwt=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a351a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, model, checkpoint_path):\n",
    "        self.device = model.get_param('device')\n",
    "        self.l_text_seq_length = model.get_param('l_text_seq_length')\n",
    "        self.r_text_seq_length = model.get_param('r_text_seq_length')\n",
    "        self.image_tokens_per_dim = model.get_param('image_tokens_per_dim')\n",
    "        self.image_seq_length = model.get_param('image_seq_length')\n",
    "        self.epochs = 1\n",
    "        self.save_path= checkpoint_path\n",
    "        self.model_name = 'rudolph_object_detection_grounding_neg_sampling_coco_vg_nmsn_'\n",
    "        self.save_every = 10000\n",
    "        self.bs = 1\n",
    "        self.clip = 1.0\n",
    "        self.lr = 2e-5\n",
    "        self.wandb = False\n",
    "        self.lt_loss_weight = 0.01\n",
    "        self.img_loss_weight = 0.01\n",
    "        self.rt_loss_weight = 7\n",
    "        self.loc_loss_weight = 0.05\n",
    "        self.category_weight = 0\n",
    "        self.bin_size = 8\n",
    "        self.nms_loss_weight=0.1\n",
    "        self.categories_num=2\n",
    "        self.iou_threshold=0.3\n",
    "        self.conf_num_bins=10\n",
    "        self.ob_seq_len = 5\n",
    "        self.conf_loss_weight=0\n",
    "        self.cat_idx=4\n",
    "        self.conf_idx=None\n",
    "        self.loc_len=4\n",
    "        self.image_size = self.image_tokens_per_dim * self.bin_size\n",
    "        \n",
    "checkpoint_path = '../model/checkpoints/'\n",
    "args = Args(model, checkpoint_path)\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "712afe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, model, checkpoint_path):\n",
    "        self.device = model.get_param('device')\n",
    "        self.l_text_seq_length = model.get_param('l_text_seq_length')\n",
    "        self.r_text_seq_length = model.get_param('r_text_seq_length')\n",
    "        self.image_tokens_per_dim = model.get_param('image_tokens_per_dim')\n",
    "        self.image_seq_length = model.get_param('image_seq_length')\n",
    "        self.epochs = 1\n",
    "        self.save_path= checkpoint_path\n",
    "        self.model_name = 'rudolph_object_detection_grounding_neg_sampling_coco_vg_nmsn_conf_'\n",
    "        self.save_every = 10000\n",
    "        self.bs = 1\n",
    "        self.clip = 1.0\n",
    "        self.lr = 2e-5\n",
    "        self.wandb = False\n",
    "        self.lt_loss_weight = 0.01\n",
    "        self.img_loss_weight = 0.01\n",
    "        self.rt_loss_weight = 7\n",
    "        self.loc_loss_weight = 0.05\n",
    "        self.category_weight = 0\n",
    "        self.bin_size = 8\n",
    "        self.nms_loss_weight=0.1\n",
    "        self.categories_num=2\n",
    "        self.iou_threshold=0.3\n",
    "        self.conf_num_bins=10\n",
    "        self.ob_seq_len = 6\n",
    "        self.conf_loss_weight=0.\n",
    "        self.cat_idx=4\n",
    "        self.conf_idx=5\n",
    "        self.loc_len=4\n",
    "        self.image_size = self.image_tokens_per_dim * self.bin_size\n",
    "        \n",
    "checkpoint_path = '../model/checkpoints/'\n",
    "args = Args(model, checkpoint_path)\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4aab397",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPC_TOKENS = {\n",
    "    '<LT_UNK>': 16384,\n",
    "    '<RT_UNK>': 16385,\n",
    "    '<LT_T2I>': 16386,\n",
    "    '<LT_I2T>': 16387,\n",
    "    '<LT_T2T>': 16388,\n",
    "    '<RT_I2T>': 16389,\n",
    "    \n",
    "    '<LT_TQA>': 16390,\n",
    "    '<RT_TQA>': 16391,\n",
    "    '<LT_ODG>': 16392,\n",
    "    '<RT_ODG>': 16393,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2659876",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(args.image_tokens_per_dim):\n",
    "    n_token = SPC_TOKENS['<RT_ODG>']+1+n\n",
    "    SPC_TOKENS['<RT_OD'+str(n)+'>'] = n_token\n",
    "last_spc_token = n_token\n",
    "\n",
    "SPC_TOKENS['<RT_NB>']= last_spc_token+1\n",
    "SPC_TOKENS['<RT_NL>']= last_spc_token+2\n",
    "last_spc_token = SPC_TOKENS['<RT_NL>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87b488cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(args.conf_num_bins):\n",
    "    n_token = last_spc_token+1+n\n",
    "    SPC_TOKENS['<RT_OD_CONF'+str(n)+'>'] = n_token\n",
    "last_spc_token = n_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50ace7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPC_TOKENS['<RT_OD_CONF_UNK>'] = last_spc_token+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab8d5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = T.Compose([\n",
    "    T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    T.ToTensor(),\n",
    "    T.Resize((args.image_size, args.image_size))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b82284a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = np.load('../../data/coco_exclude.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1202f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoVisualGenomeDataset(Dataset):\n",
    "    spc_id = -1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        rus_categories_list,\n",
    "        coco_exclude_list,\n",
    "        coco_root: str,\n",
    "        coco_ann_file: str,\n",
    "        vg_annotation_path: str,\n",
    "        vg_images_root: str,\n",
    "        args,\n",
    "        image_transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.coco_root = coco_root\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "        self.image_size = args.image_size\n",
    "        self.bin_size = args.bin_size\n",
    "        self.r_text_seq_length = args.r_text_seq_length\n",
    "        self.l_text_seq_length = args.l_text_seq_length\n",
    "        self.image_tokens_per_dim = args.image_tokens_per_dim\n",
    "        \n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_eos_id = tokenizer.eos_id\n",
    "        self.tokenizer_bos_id = tokenizer.bos_id\n",
    "        self.spc_tokens = SPC_TOKENS\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        text_prompt = 'Ð½Ð°Ð¹Ð´Ð¸ Ð½Ð° ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐµ Ð²ÑÐµ '\n",
    "\n",
    "        self.coco = COCO(coco_ann_file)\n",
    "        self.idxs = list(sorted(self.coco.imgs.keys()))\n",
    "        for ex_img in coco_exclude_list:\n",
    "            self.idxs.remove(ex_img)\n",
    "         \n",
    "        \n",
    "        categories = self.coco.dataset['categories']\n",
    "        cat_idxs = list([cat['id'] for cat in categories])\n",
    "        self.category_encoding_map = {}\n",
    "        for i, cat in enumerate(categories):\n",
    "            self.category_encoding_map[cat['id']] = self.encode_text(text=text_prompt+rus_categories_list[i],\n",
    "                                                                text_seq_length=args.l_text_seq_length)\n",
    "        \n",
    "        \n",
    "        for idx in self.idxs:\n",
    "            anns = self.coco.loadAnns(self.coco.getAnnIds(idx))\n",
    "            anns_df = pd.DataFrame(anns)\n",
    "            if len(anns_df.columns)>0:\n",
    "                cat_id_set = set(anns_df['category_id'])\n",
    "                for cat_id in cat_id_set:\n",
    "                    coco_image = self.coco.loadImgs(idx)[0][\"file_name\"]\n",
    "                    new_sample = {'encoded_left':self.category_encoding_map[cat_id],\n",
    "                                 'bboxes':anns_df[anns_df['category_id']==cat_id]['bbox'].tolist(),\n",
    "                                'image_address':os.path.join(self.coco_root, coco_image)}\n",
    "                    self.data.append(new_sample)\n",
    "        \n",
    "        self.vg_images_root = vg_images_root\n",
    "        with open(vg_annotation_path, 'rb') as fj:\n",
    "            vg_annotation = json.load(fj)\n",
    "        \n",
    "        for image_name in vg_annotation.keys():\n",
    "            if image_name in os.listdir(self.vg_images_root):\n",
    "                ann_img = vg_annotation[image_name]\n",
    "                for phrase in ann_img.keys():\n",
    "                    new_sample = {'encoded_left':self.encode_text(text=text_prompt+phrase,\n",
    "                                                                text_seq_length=args.l_text_seq_length),\n",
    "                                 'bboxes':[ann_img[phrase][0]],\n",
    "                                'image_address':os.path.join(self.vg_images_root, image_name)}\n",
    "                    self.data.append(new_sample)\n",
    "            \n",
    "    def _load_image(self, image_address) -> Image.Image:\n",
    "        return Image.open(image_address)\n",
    "    \n",
    "    def resize_bbox(self, bbox, img_size):\n",
    "        \n",
    "        bbox_x0 = bbox[0]\n",
    "        bbox_y0 = bbox[1]\n",
    "        bbox_x1 = bbox_x0+bbox[2]\n",
    "        bbox_y1 = bbox_y0+bbox[3]\n",
    "        \n",
    "        img_w = img_size[0]\n",
    "        img_h = img_size[1]\n",
    "        \n",
    "        max_wh = max(img_w,img_h)\n",
    "\n",
    "        resized_x0 = int(bbox_x0*self.image_size/img_w)\n",
    "        resized_y0 = int(bbox_y0*self.image_size/img_h)\n",
    "        resized_x1 = int(bbox_x1*self.image_size/img_w)\n",
    "        resized_y1 = int(bbox_y1*self.image_size/img_h)\n",
    "        return [resized_x0,resized_y0,resized_x1, resized_y1]\n",
    "    \n",
    "    def encode_bbox(self, resized_bbox):\n",
    "        \n",
    "        bbox_x0_bin = resized_bbox[0]//self.bin_size\n",
    "        bbox_y0_bin = resized_bbox[1]//self.bin_size\n",
    "        bbox_x1_bin = min(resized_bbox[2]//self.bin_size, self.image_tokens_per_dim-1)\n",
    "        bbox_y1_bin = min(resized_bbox[3]//self.bin_size, self.image_tokens_per_dim-1)\n",
    "        \n",
    "        new_bbox_tokens = [SPC_TOKENS['<RT_OD'+str(bbox_x0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_x1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y1_bin)+'>']]\n",
    "        \n",
    "        return new_bbox_tokens\n",
    "    \n",
    "    def create_new_box(self):\n",
    "        bbox_x0_bin = random.randint(0, self.args.image_tokens_per_dim-3)\n",
    "        bbox_y0_bin = random.randint(0, self.args.image_tokens_per_dim-3)\n",
    "        bbox_x1_bin = random.randint(bbox_x0_bin, self.args.image_tokens_per_dim-1)\n",
    "        bbox_y1_bin = random.randint(bbox_y0_bin, self.args.image_tokens_per_dim-1)\n",
    "        new_bbox_tokens = [SPC_TOKENS['<RT_OD'+str(bbox_x0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y0_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_x1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_OD'+str(bbox_y1_bin)+'>'],\n",
    "                           SPC_TOKENS['<RT_NL>'],\n",
    "                           SPC_TOKENS['<RT_OD_CONF_UNK>']\n",
    "                          ]\n",
    "        return new_bbox_tokens\n",
    "    \n",
    "    def encode_target(self, bboxes, img_size):\n",
    "        encoded_target = [self.tokenizer_bos_id, SPC_TOKENS['<RT_ODG>']]\n",
    "        ground_truth = []\n",
    "        \n",
    "        for i,bbox in enumerate(bboxes):\n",
    "            \n",
    "            resized_bbox = self.resize_bbox(bbox, img_size)\n",
    "            encoded_bbox = self.encode_bbox(resized_bbox)\n",
    "            \n",
    "            ground_truth += resized_bbox\n",
    "            ground_truth += [0]\n",
    "            \n",
    "            for loc_token in encoded_bbox:\n",
    "                encoded_target.append(loc_token)\n",
    "            encoded_target.append(SPC_TOKENS['<RT_NB>'])\n",
    "            encoded_target.append(SPC_TOKENS['<RT_OD_CONF_UNK>'])\n",
    "        for k in range((self.r_text_seq_length-2-len(encoded_target))//args.ob_seq_len+1):\n",
    "            encoded_target += self.create_new_box()\n",
    "        \n",
    "        return torch.Tensor(encoded_target), torch.Tensor(ground_truth)\n",
    "    \n",
    "    def encode_text(self, text, text_seq_length, add_special = True):\n",
    "        tokens = self.tokenizer.tokenizer.encode([text], output_type=yttm.OutputType.ID)[0]\n",
    "        bos = [self.tokenizer.bos_id]\n",
    "        if add_special:\n",
    "            bos.append(self.spc_id)\n",
    "        tokens = bos + tokens + [self.tokenizer.eos_id]\n",
    "        return self.tokenizer.prepare_tokens(tokens, text_seq_length)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = self.data[index]\n",
    "        left_encoded_text = sample['encoded_left']\n",
    "        bboxes = sample['bboxes']\n",
    "        image_address = sample['image_address']\n",
    "        \n",
    "        image = self._load_image(image_address)\n",
    "        img_size = image.size\n",
    "        transformed_image = self.image_transform(image)\n",
    "        \n",
    "        left_special_token = '<LT_ODG>'\n",
    "        right_special_token = '<RT_ODG>'\n",
    "        \n",
    "        left_encoded_text[torch.where(left_encoded_text == self.spc_id)] = self.spc_tokens[left_special_token]\n",
    "        \n",
    "        encoded_target, ground_truth = self.encode_target(bboxes, img_size)\n",
    "\n",
    "        return left_encoded_text, transformed_image, encoded_target, ground_truth\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c828c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_rus_names = [\"Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº\",\"Ð²ÐµÐ»Ð¾ÑÐ¸Ð¿ÐµÐ´\",\"Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÑŒ\",\"Ð¼Ð¾Ñ‚Ð¾Ñ†Ð¸ÐºÐ»\",\"ÑÐ°Ð¼Ð¾Ð»Ñ‘Ñ‚\",\"Ð°Ð²Ñ‚Ð¾Ð±ÑƒÑ\",\"Ð¿Ð¾ÐµÐ·Ð´\",\"Ð³Ñ€ÑƒÐ·Ð¾Ð²Ð¸Ðº\",\"Ð»Ð¾Ð´ÐºÐ°\",\n",
    "                 \"ÑÐ²ÐµÑ‚Ð¾Ñ„Ð¾Ñ€\",\"Ð³Ð¸Ð´Ñ€Ð°Ð½Ñ‚\",\"Ð·Ð½Ð°Ðº ÑÑ‚Ð¾Ð¿\",\"Ð¿Ð°Ñ€ÐºÐ¾Ð²Ð¾Ñ‡Ð½Ñ‹Ð¹ ÑÑ‡ÐµÑ‚Ñ‡Ð¸Ðº\",\"ÑÐºÐ°Ð¼ÐµÐ¹ÐºÐ°\",\"Ð¿Ñ‚Ð¸Ñ†Ð°\",\"ÐºÐ¾ÑˆÐºÐ°\",\"ÑÐ¾Ð±Ð°ÐºÐ°\",\n",
    "                \"Ð»Ð¾ÑˆÐ°Ð´ÑŒ\",\"Ð¾Ð²Ñ†Ð°\",\"ÐºÐ¾Ñ€Ð¾Ð²Ð°\",\"ÑÐ»Ð¾Ð½\",\"Ð¼ÐµÐ´Ð²ÐµÐ´ÑŒ\",\"Ð·ÐµÐ±Ñ€Ð°\",\"Ð¶Ð¸Ñ€Ð°Ñ„\",\"Ñ€ÑŽÐºÐ·Ð°Ðº\",\"Ð·Ð¾Ð½Ñ‚\",\"ÑÑƒÐ¼Ð¾Ñ‡ÐºÐ°\",\"Ð³Ð°Ð»ÑÑ‚ÑƒÐº\",\n",
    "                \"Ñ‡ÐµÐ¼Ð¾Ð´Ð°Ð½\",\"Ñ„Ñ€Ð¸ÑÐ±Ð¸\",\"Ð»Ñ‹Ð¶Ð¸\",\"ÑÐ½Ð¾ÑƒÐ±Ð¾Ñ€Ð´\",\"ÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð¼ÑÑ‡\",\"Ð²Ð¾Ð·Ð´ÑƒÑˆÐ½Ñ‹Ð¹ Ð·Ð¼ÐµÐ¹\",\"Ð±ÐµÐ¹ÑÐ±Ð¾Ð»ÑŒÐ½Ð°Ñ Ð±Ð¸Ñ‚Ð°\",\n",
    "                 \"Ð±ÐµÐ¹ÑÐ±Ð¾Ð»ÑŒÐ½Ð°Ñ Ð¿ÐµÑ€Ñ‡Ð°Ñ‚ÐºÐ°\",\"ÑÐºÐµÐ¹Ñ‚Ð±Ð¾Ñ€Ð´\",\"Ð´Ð¾ÑÐºÐ° Ð´Ð»Ñ ÑÑ‘Ñ€Ñ„Ð¸Ð½Ð³Ð°\",\"Ñ‚ÐµÐ½Ð½Ð¸ÑÐ½Ð°Ñ Ñ€Ð°ÐºÐµÑ‚ÐºÐ°\",\"Ð±ÑƒÑ‚Ñ‹Ð»ÐºÐ°\",\"Ð±Ð¾ÐºÐ°Ð» Ð´Ð»Ñ Ð²Ð¸Ð½Ð°\",\n",
    "                 \"Ñ‡Ð°ÑˆÐºÐ°\",\"Ð²Ð¸Ð»ÐºÐ°\",\"Ð½Ð¾Ð¶\",\"Ð»Ð¾Ð¶ÐºÐ°\",\"Ð¼Ð¸ÑÐºÐ°\",\"Ð±Ð°Ð½Ð°Ð½\",\"ÑÐ±Ð»Ð¾ÐºÐ¾\",\"ÑÑÐ½Ð´Ð²Ð¸Ñ‡\",\"Ð°Ð¿ÐµÐ»ÑŒÑÐ¸Ð½\",\"Ð±Ñ€Ð¾ÐºÐºÐ¾Ð»Ð¸\",\"Ð¼Ð¾Ñ€ÐºÐ¾Ð²ÑŒ\",\n",
    "                 \"Ñ…Ð¾Ð´-Ð´Ð¾Ð³\",\"Ð¿Ð¸Ñ†Ñ†Ð°\",\"Ð¿Ð¾Ð½Ñ‡Ð¸Ðº\",\"Ñ‚Ð¾Ñ€Ñ‚\",\"ÑÑ‚ÑƒÐ»\",\"Ð´Ð¸Ð²Ð°Ð½\",\"Ñ€Ð°ÑÑ‚ÐµÐ½Ð¸Ðµ Ð² Ð³Ð¾Ñ€ÑˆÐºÐµ\",\"ÐºÑ€Ð¾Ð²Ð°Ñ‚ÑŒ\",\"Ð¾Ð±ÐµÐ´ÐµÐ½Ð½Ñ‹Ð¹ ÑÑ‚Ð¾Ð»\",\"Ñ‚ÑƒÐ°Ð»ÐµÑ‚\",\"Ñ‚ÐµÐ»ÐµÐ²Ð¸Ð·Ð¾Ñ€\",\n",
    "                 \"Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐº\",\"ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ð°Ñ Ð¼Ñ‹ÑˆÑŒ\",\"Ð¿ÑƒÐ»ÑŒÑ‚\",\"ÐºÐ»Ð°Ð²Ð¸Ð°Ñ‚ÑƒÑ€Ð°\",\"Ð¼Ð¾Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½\",\"Ð¼Ð¸ÐºÑ€Ð¾Ð²Ð¾Ð»Ð½Ð¾Ð²ÐºÐ°\",\"Ð´ÑƒÑ…Ð¾Ð²ÐºÐ°\",\n",
    "                 \"Ñ‚Ð¾ÑÑ‚ÐµÑ€\",\"Ñ€Ð°ÐºÐ¾Ð²Ð¸Ð½Ð°\",\"Ñ…Ð¾Ð»Ð¾Ð´Ð¸Ð»ÑŒÐ½Ð¸Ðº\",\"ÐºÐ½Ð¸Ð³Ð°\",\"Ñ‡Ð°ÑÑ‹\",\"Ð²Ð°Ð·Ð°\",\"Ð½Ð¾Ð¶Ð½Ð¸Ñ†Ñ‹\",\"Ð¿Ð»ÑŽÑˆÐµÐ²Ñ‹Ð¹ Ð¼Ð¸ÑˆÐºÐ°\",\"Ñ„ÐµÐ½\",\"Ð·ÑƒÐ±Ð½Ð°Ñ Ñ‰Ñ‘Ñ‚ÐºÐ°\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fe50942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.16s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CocoVisualGenomeDataset(coco_root=r'../../data/train2017',\n",
    "                             coco_ann_file=r'../../data/annotations/instances_train2017.json',\n",
    "                             image_transform=image_transform,\n",
    "                             tokenizer=tokenizer,\n",
    "                             args = args,\n",
    "                             coco_exclude_list=exclude_list,\n",
    "                             rus_categories_list=cat_rus_names,\n",
    "                                      vg_annotation_path=r'../../data/vg_intersection_rus_train.json',\n",
    "                                      vg_images_root = r'../../data/VG_100K'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8c0de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(tokenizer, encoded, ignore_ids):\n",
    "    return tokenizer.tokenizer.decode(encoded.cpu().numpy().tolist(), ignore_ids=ignore_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "090371e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_ids = [\n",
    "    tokenizer.eos_id, tokenizer.bos_id, tokenizer.unk_id, tokenizer.pad_id,\n",
    "    -1, *list(SPC_TOKENS.values())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cf3b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=args.bs, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50b08fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "left, img ,tgt, _ = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a4a27a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ð½Ð°Ð¹Ð´Ð¸ Ð½Ð° ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐµ Ð²ÑÐµ Ð±Ð°Ð½Ð°Ð½'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text(tokenizer, left, ignore_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77ecae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False,\n",
    "):\n",
    "    for name, p in model.module.named_parameters():\n",
    "        name = name.lower()\n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            p.requires_grad = not freeze_ff\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a7175ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename='checkpoint.pt'):\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80399ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=args.lr, final_div_factor=500, \n",
    "    steps_per_epoch=len(train_dataloader), epochs=args.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2937807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_right(right_tokens, length=None, scores=True, ob_seq_len=5, cat_idx=4, loc_len=4):\n",
    "    boxes = torch.zeros((length, loc_len))\n",
    "    labels = torch.ones(length)\n",
    "    scores = torch.zeros(length)\n",
    "    \n",
    "    for k in range(length):\n",
    "        boxes[k,:] = right_tokens[k*ob_seq_len:k*ob_seq_len+cat_idx].unsqueeze(0).cpu()\n",
    "        scores[k] = 1\n",
    "        \n",
    "    boxes -= SPC_TOKENS['<RT_OD0>']*torch.ones((length, loc_len))\n",
    "    boxes *= args.bin_size\n",
    "\n",
    "    return [{'boxes':boxes, 'labels':labels, 'scores':scores}]\n",
    "\n",
    "def decode_ground_truth(ground_truth, length=None, scores=True, ob_seq_len=5, cat_idx=4, ):\n",
    "    boxes = torch.zeros((length, 4))\n",
    "    labels = (SPC_TOKENS['<RT_NL>']-SPC_TOKENS['<RT_NB>'])*torch.ones(length)\n",
    "    scores = torch.zeros(length)\n",
    "    \n",
    "    for k in range(len(ground_truth)//ob_seq_len):\n",
    "        labels[k] = ground_truth[k*ob_seq_len+cat_idx].unsqueeze(0).cpu()\n",
    "        boxes[k,:] = ground_truth[k*ob_seq_len:k*ob_seq_len+cat_idx].unsqueeze(0).cpu()\n",
    "        scores[k] = 1\n",
    "\n",
    "    return [{'boxes':boxes, 'labels':labels, 'scores':scores}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68818cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, args: Args, train_dataloader, from_checkpoints = False):\n",
    "    \"\"\"\n",
    "      args - arguments for training\n",
    "\n",
    "      train_dataloader - VisualGenomeDataset\n",
    "      \"\"\"\n",
    "    loss_logs = []\n",
    "    encoded_right_text = None\n",
    "    #try:\n",
    "    t_steps = len(train_dataloader)*args.epochs\n",
    "    progress = tqdm(total = t_steps, desc='ðŸ¦ŒðŸ¦ŒðŸ¦Œfinetuning processðŸ¦ŒðŸ¦ŒðŸ¦Œ')\n",
    "\n",
    "    save_counter = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    if from_checkpoints:\n",
    "        save_counter = max([int(f.split('_')[-1].split('.')[0]) for f in os.listdir(args.save_path) if f.startswith(args.model_name)])\n",
    "        model, optimizer, start_epoch, loss_logs = load_checkpoint(model, optimizer, loss_logs, filename=os.path.join(args.save_path,f\"{args.model_name}state_{save_counter}.pt\"))\n",
    "        start_epoch -= 1\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "        for encoded_left_text, images, encoded_right_text, _ in train_dataloader: #, prompt, image, locs\n",
    "            \n",
    "            bs_text = encoded_right_text.shape[0]\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            image_seq_length = args.image_tokens_per_dim ** 2\n",
    "            total_seq_length = args.l_text_seq_length + image_seq_length + args.r_text_seq_length\n",
    "            encoded_images = vae.get_codebook_indices(images, disable_gumbel_softmax=True)\n",
    "\n",
    "            attention_mask_text = get_attention_mask(bs_text, args.l_text_seq_length,\n",
    "                                             args.image_tokens_per_dim, \n",
    "                                             args.r_text_seq_length, args.device)\n",
    "            input_ids_text = torch.cat((encoded_left_text.to(args.device).int(),\n",
    "                                encoded_images.int(),\n",
    "                                encoded_right_text.to(args.device).int()), dim=1)\n",
    "            if input_ids_text.size(1)==args.l_text_seq_length+args.image_tokens_per_dim+args.r_text_seq_length:\n",
    "                \n",
    "                loss, loss_values = model.forward(input_ids_text.long(), attention_mask_text, \n",
    "                                                                 lt_loss_weight=args.lt_loss_weight,\n",
    "                                                                 img_loss_weight=args.img_loss_weight, \n",
    "                                                                 rt_loss_weight=args.rt_loss_weight,\n",
    "                                                  return_loss=True,\n",
    "                                                  category_weight=args.category_weight,\n",
    "                                                  fake_category_token = SPC_TOKENS['<RT_NL>'],\n",
    "                                                  loc_loss_weight=args.loc_loss_weight,\n",
    "                                                  zero_loc_token = SPC_TOKENS['<RT_OD0>'],\n",
    "                                                  nms_loss_weight=args.nms_loss_weight,\n",
    "                                                  categories_num=args.categories_num,\n",
    "                                                  iou_threshold=args.iou_threshold,\n",
    "                                                  conf_loss_weight=args.conf_loss_weight,\n",
    "                                                  iou_bin_tokens=[SPC_TOKENS['<RT_OD_CONF'+str(n)+'>'] for n in range(args.conf_num_bins)],\n",
    "                                                  cat_idx=args.cat_idx,\n",
    "                                                  conf_idx=args.conf_idx,\n",
    "                                                  loc_len=args.loc_len,\n",
    "                                                  od_seq_len=args.ob_seq_len,\n",
    "                                                  moc_iou_token=SPC_TOKENS['<RT_OD_CONF_UNK>']\n",
    "                                                 )\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                save_counter += 1\n",
    "                if save_counter % args.save_every == 0:\n",
    "                    print(f'Saving checkpoint here {args.model_name}_rudolph_{save_counter}.pt')\n",
    "                    plt.plot(loss_logs)\n",
    "                    plt.show()\n",
    "                    state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "                             'optimizer': optimizer.state_dict(), 'losslogger': loss_logs}\n",
    "                    torch.save(\n",
    "                        model.state_dict(),\n",
    "                        os.path.join(args.save_path,f\"{args.model_name}rudolph_{save_counter}.pt\")\n",
    "                    )\n",
    "                    torch.save(\n",
    "                        state,\n",
    "                        os.path.join(args.save_path,f\"{args.model_name}state_{save_counter}.pt\")\n",
    "                    )\n",
    "\n",
    "                loss_logs+=[loss.item()]\n",
    "                progress.update()\n",
    "                progress.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        print(f'Complitly tuned and saved here  {args.model_name}__object_detection_last.pt')\n",
    "        plt.plot(loss_logs)\n",
    "        plt.show()\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(args.save_path,f\"{args.model_name}object_detection_last.pt\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f9106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe31c5610ea84f26951e54f31a156ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ðŸ¦ŒðŸ¦ŒðŸ¦Œfinetuning processðŸ¦ŒðŸ¦ŒðŸ¦Œ:   0%|          | 0/419308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = freeze(\n",
    "    model=model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False,\n",
    ")\n",
    "\n",
    "train(model, optimizer, scheduler, args, train_dataloader, from_checkpoints=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
